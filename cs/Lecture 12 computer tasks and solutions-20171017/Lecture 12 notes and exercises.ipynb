{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:34f2195987bf5dcd07b91ff8f23d2fd9f1bbd11e81fadbb435876fe810d9fd23"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 12: MCMC and gradients 2: HMC tuning with NUTS and HMC for posterior inference\n",
      "\n",
      "In this lecture we will consider two topics: automatic tuning using the no-U-turn-sampler (NUTS) and applying HMC for sampling from the posterior distribution of a Bayesian model.\n",
      "\n",
      "## HMC tuning with NUTS\n",
      "\n",
      "As shown by the problems of Lecture 11, tuning the stepsize $\\epsilon$ and number of leapfrog steps $L$ in HMC can be nontrivial.\n",
      "\n",
      "The No-U-Turn Sampler (NUTS) of Hoffman and Gelman provides a neat way of avoiding this issue. Intuitively, NUTS works by iteratively extending the path until it observes a U-turn of the path turning back on itself. In order to guarantee detailed balance, the path is extended both forward and backward in virtual time. Furthermore instead of a single accept/reject choice, the algorithm samples the next point among the set of candidates generated along the path. The algorithm also includes a heuristic for tuning $\\epsilon$.\n",
      "\n",
      "Most modern Bayesian inference software packages use NUTS as the default MCMC algorithm for cases where it is applicable (models with continuous variables).\n",
      "\n",
      "## Further reading:\n",
      "\n",
      "Hoffman & Gelman: The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. JMLR 15:1593-1623 (2014).  \n",
      "http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf\n",
      "\n",
      "Radford M. Neal: MCMC using Hamiltonian Dynamics  \n",
      "https://arxiv.org/pdf/1206.1901.pdf  \n",
      "(A classical and very comprehensive HMC tutorial)\n",
      "\n",
      "Michael Betancourt: A Conceptual Introduction to Hamiltonian Monte Carlo  \n",
      "https://arxiv.org/pdf/1701.02434.pdf  \n",
      "(A more modern HMC tutorial)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 0. HMC with fixed code\n",
      "\n",
      "Rerun the examples from Lecture 11 using the fixed HMC sampler. (I am very sorry about the mistake in the original code!)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. NUTS\n",
      "\n",
      "Redo the examples from Lecture 11 using the No-U-Turn Sampler of Hoffman & Gelman. How many samples do you need to get a good representation of the target density?\n",
      "\n",
      "If you are familiar with Python, it is a good exercise to re-implement the NUTS algorithm (Algorithm 6 in Hoffman & Gelman). If you do not feel up to the task, you can try the example implementation available at\n",
      "http://www.helsinki.fi/~ahonkela/teaching/compstats1/nuts.py  \n",
      "\n",
      "Hint: to use the example code, save the file to the same directory as this notebook, run\n",
      "``` {python}\n",
      "import nuts\n",
      "nuts.nuts6(...)\n",
      "```\n",
      "You can use `? nuts.nuts6` to get help."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. HMC sampling for posterior inference in linear regression\n",
      "\n",
      "Let us revisit Problem 4 from Lecture 4 where we studied linear regression using maximum likelihood estimation.\n",
      "We will use data from the Framingham Heart Study that studies the association between heart disease and its causes.\n",
      "\n",
      "A description of the data and its fields can be found at http://www.maths.utas.edu.au/DHStat/Data/Flow.html\n",
      "\n",
      "1. Load the data using the below code. Standardise the data by subtracting the mean and dividing each variable by two standard deviations (see http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf). Fit all the models using the standardised data as this makes sampling a lot easier.\n",
      "2. Using the same normal log-likelihood as Problem 4.2 from Lecture 4 and priors\n",
      "$$ p(\\alpha) = \\mathcal{N}(\\alpha;\\; 0, 2^2) \\\\\n",
      "  p(\\beta) = \\mathcal{N}(\\beta;\\; 0, 2^2), $$\n",
      "use HMC / NUTS to draw samples from the posterior distribution. How does the posterior compare to the maximum likelihood estimates obtained at Lecture 4?\n",
      "3. Change the likelihood to use the Laplace distribution for the residuals $\\epsilon_i$ while keeping the same priors. Repeat the previous task."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import autograd.numpy as np\n",
      "import autograd\n",
      "from scipy.optimize import minimize\n",
      "\n",
      "# load the data from CSV file using pandas\n",
      "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
      "# convert the variables of interest to numpy arrays for autograd compatibility\n",
      "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group\n",
      "x = np.array(fram['FRW'])\n",
      "# target: Systolic blood pressure, examination 1\n",
      "y = np.array(fram['SBP'])\n",
      "\n",
      "def standardize(x):\n",
      "    # Remove the next line and replace with your implementation\n",
      "    raise(NotImplementedError(\"Not implemented yet\"))\n",
      "\n",
      "xs = standardize(x)\n",
      "ys = standardize(y)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. HMC sampling for posterior inference 2\n",
      "\n",
      "Apply HMC / NUTS sampling to perform Bayesian inference over the mean $\\mu$ and the standard deviation $\\sigma$ in a normal model\n",
      "$$ p(x_i | \\mu, \\sigma^2) = \\mathcal{N}(x_i;\\; \\mu, \\sigma^2) $$\n",
      "for a data set $X = (x_i)_{i=1}^n$ loaded below.\n",
      "\n",
      "We will use the same conjugate priors as in Exercise 5 for Week 4:\n",
      "$$ p(\\mu) = \\mathcal{N}(\\mu;\\; \\mu_0, \\sigma_0^2) \\\\ p(\\sigma^2) = \\mathrm{InvGamma}(\\alpha, \\beta), $$\n",
      "where $\\mathrm{InvGamma}(\\alpha, \\beta)$ denotes the inverse-gamma distribution. For standardised data, we can use the hyperparameters $\\mu_0 = 0, \\sigma_0^2 = 10^2, \\alpha = \\beta = 2$.\n",
      "\n",
      "Hints:\n",
      "* In order to apply HMC / NUTS you need to transform $\\sigma^2$ to an unbounded space with $\\log$-transformation as before.\n",
      "* In order to play safe with autograd, you may want to implement the required probability densities yourself rather than using scipy.\n",
      "* NUTS can sometimes drive some of the parameters to quite extreme values. You should make sure that your target function evaluation is numerically stable, i.e. make sure you will not evaluate exp() for too large numbers etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import autograd.numpy as np\n",
      "import autograd.numpy.random as npr\n",
      "import pandas as pd\n",
      "import autograd.scipy.special as scs\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata2.txt', sep='\\t', header=None)\n",
      "data = data.values\n",
      "data = np.array(data[:,0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}