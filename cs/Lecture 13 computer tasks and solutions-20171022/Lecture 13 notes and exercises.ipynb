{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:5645516bec480da069d25d1d9868e1050e59fac7cb26ed3a857ee7ce6e38bb34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 13: Approximate Bayesian Computation (ABC)\n",
      "\n",
      "The sampling methods like MCMC considered before are asymptotically consistent: they will eventually converge to the correct solution. This is a nice property to have, but it can also be quite restrictive from the algorithm design perspective. This week we will consider two methods, approximate Bayesian computation (ABC) and variational inferece (VI) which take a different approach: their answers may not be asymptotically consistent, but they can often provide answers significantly faster than MCMC and related methods.\n",
      "\n",
      "## ABC\n",
      "\n",
      "Let us consider a model $p(X, \\theta) = p(X | \\theta) p(\\theta)$ for some data $x$ that depends on some parameter $\\theta$.\n",
      "\n",
      "The basic idea of ABC is very simple: we sample $\\theta^* \\sim p(\\theta)$ from the prior $p(\\theta)$ and simulate a new observed data set $X$ conditional to $\\theta^*$: $X^* \\sim p(X | \\theta)$.\n",
      "\n",
      "### Exact rejection ABC\n",
      "\n",
      "In exact rejection ABC a sample $\\theta^*$ is accepted if $X^* = X$. In this case it is easy to see that the accepted samples follow the posterior distribution $p(\\theta | X)$.\n",
      "\n",
      "Exact rejection ABC is only applicable to very simple problems, because usually the probability of simulating an exact replica of the data set is negligible.\n",
      "\n",
      "### Summary statistics ABC\n",
      "\n",
      "ABC can be made more efficient by requiring that only some summary statistics $S(X)$ should match between the generated and the observed data sets. These could be e.g. means and variances of the data. In case the selected statistics are *sufficient*, such as mean and variance for the normal distribution, the sampler will still be exact.\n",
      "\n",
      "Summary statistics allow for a simple relaxation that makes the sampler approximate: we can accept proposals with $ \\| S(X) - S(X^*) \\| < \\epsilon$ for some $\\epsilon > 0$. This can make sampling significantly more efficient, especially in the case of continuous $X$ and $S(X)$, at the cost of sampling from an approximation of the posterior. It can, however, be shown that under certain assumptions, this approximate sampling corresponds to exact sampling from a related model (see Wilkinson, 2013, linked below for more details).\n",
      "\n",
      "### Further developments\n",
      "\n",
      "Even after allowing for some slack in the match of the summary statistics, independent sampling from the prior may be too inefficient. Methods coupling ABC with some form of MCMC can help.\n",
      "\n",
      "### Examples of ABC applications\n",
      "\n",
      "* Models that are easy to simulate but have computationally demanding likelihoods (e.g. phylogenetic trees for species)\n",
      "* Models based on simulators of physical/chemical systems (e.g. climate simulators)\n",
      "\n",
      "## Further reading:\n",
      "\n",
      "Mikael Sunn\u00e5ker et al.: Approximate Bayesian Computation. PLoS Computational Biology 2013.  \n",
      "https://doi.org/10.1371/journal.pcbi.1002803\n",
      "\n",
      "Richard Wilkinson and Simon Tavar\u00e9: Approximate Bayesian Computation: a simulation based approach to inference.  \n",
      "http://www0.cs.ucl.ac.uk/staff/C.Archambeau/AIS/Talks/rwilkinson_ais08.pdf\n",
      "\n",
      "Richard Wilkinson: Approximate Bayesian computation (ABC) gives exact results under the assumption of model error. Stat. App. Gen. Mol. Bio. 12 (2013) 129-141  \n",
      "https://arxiv.org/abs/0811.3355"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. Exact rejection ABC\n",
      "\n",
      "Implement an exact rejection ABC sampler to sample from the posterior distribution of the probability $p$ of obtaining heads in a coin flip when observing $r$ heads in $n$ throws:\n",
      "$$ p \\sim \\mathrm{Uniform}(0, 1), \\quad r \\sim \\mathrm{Binom}(n, p). $$\n",
      "\n",
      "1. Simulate 1000 samples from the posterior when $n = 20, r = 10$.\n",
      "2. Simulate 1000 samples from the posterior when $n = 40, r = 25$ and estimate the probability $p < 0.5$.\n",
      "3. Simulate 1000 samples from the posterior when $n = 80, r = 50$ and estimate the probability $p < 0.5$.\n",
      "\n",
      "How does the efficiency of the sampler depend on $n$? What proportion of the samples are accepted?\n",
      "\n",
      "Hint: you can use IPython/Jupyter \"magic\" %time before a command to check its runtime."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. ABC for a discrete switching time series\n",
      "\n",
      "In this task we will consider a Markov model with two states 0 and 1. The model follows\n",
      "$$ p(x_{t+1} | x_t) = \\begin{cases} 1-\\pi, \\quad \\text{if } x_{t+1} = x_t \\\\ \\pi, \\quad \\text{if } x_{t+1} \\neq x_t  \\end{cases} $$\n",
      "i.e. the state will flip with probability $\\pi$. We will assume the chain starts in state $x_0 = 0$.\n",
      "\n",
      "We set a uniform prior $p(\\pi) = \\mathrm{Uniform}(\\pi;\\; 0, 1)$\n",
      "\n",
      "Our task is to infer the posterior distribution over $\\pi$ given an observed sequence generated by the model.\n",
      "\n",
      "1. Implement an exact ABC sampler that generates full sequences and accepts if the full sequences match. Test your sampler by generating at least 100 samples from the posterior distribution given a single observed sequence '0011001100'.\n",
      "2. Implement an ABC sampler that uses the number of state flips $S(X) = \\#\\{i | x_i \\neq x_{i+1}\\}$ as the sufficient statistic. Are the two samplers sampling from the same distribution? How long sequences can each of them handle?\n",
      "3. Extend your sampler from subtask 2 by allowing acceptance when $| S(X) - S(X^*) | \\le \\epsilon$. Test this sampler and the one from subtask 2 for a sequence of length 200 that always flips state after 2 symbols, i.e. '00110011...0011'. Check how the accuracy of the posterior mean and standard deviation, and the computing time depend on $\\epsilon$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Rejection ABC with continuous data\n",
      "\n",
      "Infer the posterior for $\\mu$ and $\\sigma^2$ for a normal model $x_i \\sim N(\\mu, \\sigma^2)$ over the toy data with priors\n",
      "$$ p(\\mu) = \\mathcal{N}(\\mu_0, \\sigma_0^2), \\quad p(\\sigma^2) = \\mathrm{Gamma}(\\alpha_0, \\beta_0) $$\n",
      "with $\\mu_0 = 4, \\sigma_0^2 = \\sqrt{10}^2, \\alpha_0 = 2, \\beta_0 = 2$.\n",
      "\n",
      "We will use the mean and standard deviation of $x_i$ as the summary statistics $S(x)$ and accept samples if $\\| S(x^*) - S(x) \\|_2 < \\epsilon$.\n",
      "\n",
      "1. Generate 1000 samples from the prior, evaluate the errors in the summary statistics and plot their distribution.\n",
      "2. Compute the approximate posterior distributions using $\\epsilon = 3$, $\\epsilon = 1$ and $\\epsilon = 0.3$. Compare their means and standard deviations.\n",
      "3. (Optional) Implement a Metropolis-Hastings or a HMC sampler for the problem and check the accuracy the estimates obtained above.\n",
      "\n",
      "Note: the prior for $\\sigma^2$ is not conjugate as previously when considering this problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
      "data = data.values\n",
      "data = np.array(data[:,0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}