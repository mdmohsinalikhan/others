{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:1f4f0bd64db01d1acf504eb8bc2f7bc51319628847c22cd34b0d4ce887a5bebd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 8: Markov chain Monte Carlo in practice\n",
      "\n",
      "MCMC algorithms like Metropolis-Hastings introduced on lecture 7 are tricky to operate because they require careful tuning to work reliably, and checking whether they actually work is highly non-trivial or often impossible.\n",
      "\n",
      "## Overall workflow\n",
      "\n",
      "1. Running three or more chains starting from very different points allows checking convergence.\n",
      "2. Samples from an initial **warm-up** or burn-in period (e.g. first 50% of samples) are discarded as they depend too much on the initial point.\n",
      "3. The proposal often needs to be tuned to get fast mixing. You should first perform any tuning, then discard all those samples and run the actual analysis. Note that the optimal tuning for warm-up and stationary phases might be different.\n",
      "4. Check for convergence by comparing the chains (see below)\n",
      "5. Mix the non-warm-up samples from all chains for final analysis.\n",
      "6. Compare your inferences with results from simpler models or approximations.\n",
      "\n",
      "## Important caveats\n",
      "\n",
      "Standard MH theory assumes that the proposal does not depend on past samples. Tuning the proposal or even adaptive termination criteria can break these assumptions. There are adaptive samplers that can avoid these issues, but they are not very widely used. Ignoring all samples from the tuning/tweaking phase is usually the best choice.\n",
      "\n",
      "It is in general impossible to prove a Markov chain has converged - you can never tell if you are just stuck exploring a local mode. MCMC is in general a method for approximate inference even though some people claim it is exact.\n",
      "\n",
      "## Tuning the proposal distribution\n",
      "\n",
      "An optimal proposal matches the target as closely as possible. Usually we do not know the target very well, so generic proposals (such as Gaussian centered at current point) need to be used.\n",
      "\n",
      "The optimal acceptance rate for Metropolis-Hastings is quite low: 50% (low dimensional) down to 23% (high dimensional). Usually short steps imply high acceptance, long steps low acceptance.\n",
      "\n",
      "## Diagnostics of sampling efficiency\n",
      "\n",
      "* Autocorrelation plot (Koistinen, Sec. 7.7)\n",
      "  $$ \\rho_k = \\frac{E[(X_i - \\mu)(X_{i+k} - \\mu)]}{\\sigma^2}, \\quad k = 0, 1, 2, \\dots $$\n",
      "* Effective sample size (Koistinen, Sec. 7.8) from $M$ samples\n",
      "  $$ \\mathrm{ESS} = \\frac{M}{1 + 2 \\sum_{k=1}^{M-1} (1 - \\frac{k}{M}) \\rho_k} $$\n",
      "\n",
      "## Convergence diagnostics\n",
      "\n",
      "* Potential scale reduction (R-hat, Gelman & Rubin, 1992)\n",
      "* Assume $x_{ij}$ is the $i$th sample (out of $n$) from chain $j$ (out of $m$)\n",
      "* Estimate within-chain variance $W$ and between-chains variance $B$\n",
      "* $ W = \\frac{1}{m} \\sum_{i=1} s_i^2 $, where $ s_i^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x_j})^2 $\n",
      "* $ B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{x}_j - \\bar{\\bar{x}})^2$, where $\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}$ and $\\bar{\\bar{x}} = \\frac{1}{m} \\sum_{j=1}^m \\bar{x}_j$\n",
      "* $ \\mathrm{Var}(x) = \\left( 1 - \\frac{1}{n} \\right) W + \\frac{1}{n} B $\n",
      "* $ \\sqrt{\\hat{R}} = \\sqrt{\\frac{\\mathrm{Var}(x)}{W}} $\n",
      "* Target $\\hat{R}$ less than 1.1 or 1.2\n",
      "* Note: $\\hat{R}$ is defined for scalar variables. For vectors it is common to compute it for each component and take the maximum over different components.\n",
      "\n",
      "## General debugging tips\n",
      "\n",
      "* Generate synthetic data from your model with known parameters and check if your inferences are consistent with those parameters.\n",
      "\n",
      "## Further reading:\n",
      "\n",
      "Koistinen: Computational Statistics (Sec. 7.7-7.9)  \n",
      "http://www.mcmchandbook.net/HandbookChapter6.pdf (Non-math intro)  \n",
      "http://www.stat.columbia.edu/~gelman/research/published/w1.pdf (More math)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Tuning a Metropolis-Hastings sampler\n",
      "\n",
      "Let us consider Metropolis-Hastings sampling of $\\mathcal{N}(0, 1)$ target with $q(x', x) = \\mathcal{N}(x', x, \\sigma^2)$ as the proposal. Your aim is to tune the proposal width $\\sigma$ to obtain optimal mixing.\n",
      "\n",
      "1. Implement the sampler and run it with different values of $\\sigma$. Record the acceptance rates and ESS values. What is the optimal $\\sigma$?\n",
      "2. Repeat the task with $q(x', x) = \\mathrm{Uniform}(x-\\sigma/2, x+\\sigma/2)$ as the proposal. What is the optimal $\\sigma$?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Metropolis-Hastings sampling for the posterior distribution of a probabilistic model (NEW!)\n",
      "\n",
      "This exercise will illustrate a very simple example of Bayesian modelling: estimating the mean $\\mu$ of a normal distribution for some observed data $X = (x_i)_{i=1}^n$ that are assumed to be independent given $\\mu$.\n",
      "\n",
      "We assume $p(x_i | \\mu) = \\mathcal{N}(x_i;\\; \\mu, \\sigma^2)$, i.e. the mean of $x_i$ is $\\mu$ and the variance $\\sigma^2 = 2$ is assumed to be fixed.\n",
      "\n",
      "Our goal is to compute $p(\\mu | X)$. We can obtain this from the Bayes rule by specifying the prior $p(\\mu)$. This will yield\n",
      "$$ p(\\mu | X) = \\frac{p(X | \\mu) p(\\mu)}{p(X)} = \\frac{\\prod_{i=1}^n p(x_i | \\mu) p(\\mu)}{p(X)}. $$\n",
      "\n",
      "1. Assume $p(\\mu) = \\mathcal{N}(\\mu;\\; 0, \\sigma_0^2).$ Implement a Metropolis-Hastings sampler to sample from $p(\\mu | X)$ using a normal proposal and $\\sigma_0^2 = 100$. Summarise your samples by computing their mean and standard deviation. Compare your result with the mean of $X$. Hint: the numerator of the Bayes rule will give you the target distribution as a function of $\\mu$.\n",
      "2. Compare the samples you obtained with the exact solution (Sec. 1.2 of https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf, but note the missing bar in $\\bar{x}$) of\n",
      "$$ p(\\mu | X) = \\mathcal{N}\\left(\\mu;\\; \\frac{\\sigma_0^2}{\\frac{\\sigma^2}{n}+\\sigma_0^2} \\bar{x}, \\left( \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}  \\right)^{-1}\\right). $$\n",
      "3. Repeat the experiment with $\\sigma_0^2 = 1$. Can you still match the exact solution?\n",
      "4. Try the sampler with $p(\\mu) = \\mathrm{Laplace}(\\mu;\\; 0, b_0) = \\frac{1}{2b_0} \\exp(-|\\mu|/b_0)$ with $b_0=10$. Summarise your samples by computing their mean and standard deviation. Note that in this case there is no easily available exact solution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import pandas as pd\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
      "data = data.values\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Sampling a multimodal distribution, convergence checking\n",
      "\n",
      "In this exercise we consider Metropolis-Hastings sampling from a two-dimensional unnormalised target distribution\n",
      "$$ P^*(x; R) = \\sum_{i=1}^5 \\exp\\left( -\\frac{1}{2} (x-\\mu_i)^T (x - \\mu_i) \\right) $$\n",
      "with $\\mu_1 = (0, 0), \\mu_2 = (R, R), \\mu_3 = (R, -R), \\mu_4 = (-R, R), \\mu_5 = (-R, -R)$. Here $R$ denotes a parameter that specifies the spread of the modes of the distribution.\n",
      "\n",
      "1. Plot a contour plot $\\log P^*(x; R)$ for $R = 6$.\n",
      "2. Write a Metropolis-Hastings sampler for the target using $q(x' ; x) = \\mathcal{N}(0, I_2)$ as the proposal and run it to obtain 10000 samples.\n",
      "3. Plot a scatter plot of the samples and compare it to the contour plot. Is the sampler exploring the density well?\n",
      "4. Run at least 3 samplers starting from different initial points and compute the R-hat statistics for both variables. What do they tell?\n",
      "5. Try repeating the experiment with $R=3$. What do the R-hat statistics look like now?\n",
      "6. Try repeating the experiment with $R=100$ using the same initial points as with $R=3$. What do the R-hat statistics look like now?\n",
      "7. Try to get the sampler to produce acceptable R-hat values for $R=6$. Try running the sampler for longer, changing the proposal, ..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Evaluating efficiency diagnostics\n",
      "\n",
      "Plot the autocorrelation plot and compute the ESS for the different proposals in Exercise 1 from lecture 7. At which point does the autocorrelation curve first cross zero?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}