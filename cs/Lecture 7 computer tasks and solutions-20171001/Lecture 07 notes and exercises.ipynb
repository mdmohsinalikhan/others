{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:0c2b9d1967ae946fc83fc101bf958692e3a9550a07b911eda52d15cabc588fa2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 7: Markov chain Monte Carlo (MCMC) basics\n",
      "\n",
      "MCMC sampling and especially the Metropolis-Hastings (MH) algorithm are ubiquitous tools in computational Bayesian statistics. MH is a powerful and versatile tool for drawing samples from a distribution which may only be known up to an unknown normalising constant, i.e. when\n",
      "$$ P(x) = \\frac{P^*(x)}{Z}, $$\n",
      "where $Z$ is unknown while $P^*(x)$ is known. This situation arises often in Bayesian inference where one wants to draw samples from the posterior distribution $p(\\theta | x)$ of some unknown quantities $\\theta$ such as model parameters, given some observed data $x$. The posterior can be evaluated using the Bayes rule\n",
      "$$ p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{p(x)} = \\frac{p(x | \\theta) p(\\theta)}{\\int_{\\boldsymbol{\\Theta}} p(x | \\theta) p(\\theta)} \\mathrm{d}\\theta, $$\n",
      "where the likelihood $p(x | \\theta)$ and the prior $p(\\theta)$ are known so the numerator is tractable, but the high-dimensional integral makes the denominator intractable. In this case the unnormalised target $P^*(\\theta) = p(x | \\theta) p(\\theta) = p(x, \\theta)$.\n",
      "\n",
      "MH sampling works by deriving a Markov chain $x^{(1)}, x^{(2)}, \\dots$ whose stationary distribution is the target $P(x)$. The algorithm works by proposing a new state $x'$ from a proposal density $Q(x' ; x^{(t)})$. The proposal is either accepted or rejected based on the quantity\n",
      "$$ a = \\frac{P^*(x')}{P^*(x^{(t)})} \\frac{Q(x^{(t)} ; x')}{Q(x' ; x^{(t)})} $$\n",
      "as follows:\n",
      "* If $a \\ge 1$ then the new state is accepted.\n",
      "* Otherwise, the new state is accepted with probability $a$.\n",
      "\n",
      "If the step is accepted, we set $x^{(t+1)} = x'$.  \n",
      "If the step is rejected, then we set $x^{(t+1)} = x^{(t)}$.  \n",
      "\n",
      "Note: If the proposal is a jump whose length does not depend on the current position, the ratio $\\frac{Q(x^{(t)} ; x')}{Q(x' ; x^{(t)})}$ will cancel. This special case is often called the Metropolis algorithm.\n",
      "\n",
      "## MCMC convergence theory\n",
      "\n",
      "The theoretical convergence of MH is not very sensitive to the proposal distribution - in theory all that is required that any part of the target space can be reached from any starting point in a finite number of steps with positive probability. In practice the speed of convergence is quite sensitive to the proposal distribution and a suboptimal proposal distribution may lead to extremely slow convergence. (It is easy to come up with quite reasonable examples where the chain will not get anywhere near convergence before the sun turns into a supernova, so this is a real concern.)\n",
      "\n",
      "In order to prove convergence to a specific distribution, a Markov chain with transition kernel $K(\\theta, A) = P(\\theta^{(t+1)} \\in A | \\theta^{(t)} = \\theta)$ defined in space $S$ can be shown to have $\\pi(\\theta)$ as its stationary distribution if it satisfies the detailed balance equations\n",
      "$$ \\int_A \\pi(\\theta) K(\\theta, B) \\mathrm{d}\\theta = \\int_B \\pi(\\phi) K(\\phi, A) \\mathrm{d}\\phi \\quad \\forall A, B \\subset S. $$\n",
      "\n",
      "## Further reading\n",
      "\n",
      "Sec. 7.1-7.4 in Computational Statistics by Koistinen.  \n",
      "Chapter 11 in Computational Statistics by Koistinen.\n",
      "\n",
      "#### Implementation hints:\n",
      "\n",
      "* It is numerically more stable to compute\n",
      "$$ \\log a = \\log P^*(x') - \\log P^*(x^{(t)}) + \\log Q(x^{(t)} ; x') - \\log Q(x' ; x^{(t)}) $$\n",
      "and exponentiate it only at the end.\n",
      "* It is usually computationally more efficient to pre-allocate a numpy.array() for the results using something like `np.zeros(nsamples)` and then fill that in a for-loop."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. Metropolis-Hastings sampling of a 1D target\n",
      "\n",
      "1. Write a Metropolis-Hastings sampler to draw samples from the standard zero mean unit variance normal distribution $\\mathcal{N}(0, 1)$ using the $Q(x' ; x) = \\mathrm{Uniform}(x-0.5, x+0.5)$ distribution as the proposal. Draw 1000 samples and compute the acceptance rate of your sampler.\n",
      "2. Plot a normed histogram of the samples you have drawn together with the true density.\n",
      "3. Plot a trace plot of the samples: a line curve of the samples in order. Do consecutive samples appear independent or are they correlated? Plot a similar curve of the samples when they are permuted to a random order. Do the curves look simiar?\n",
      "4. Try increasing the number of samples to 10000 and only including every 10th sample in the plots. This is called *thinning*. (Hint: you can extract every 10th element of a numpy.array x using `x[0:10000:10]`.)\n",
      "5. Try changing the bounds of the proposal distribution such that they remain symmetric about 0, run the sampler and redraw the plots. Can you make the samples appear more independent? Is there a connection between the acceptance rate and the appearance of the plots?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. Metropolis-Hastings sampling of constrained parameters using transformations\n",
      "\n",
      "In this task we will develop a Metropolis-Hastings sampler for Bayesian inference of the variance of normally distributed data. This will demonstrate the use of MH sampling for Bayesian inference as well as using transformations to enforce the positivity of the variance parameter.\n",
      "\n",
      "The probabilistic model is as follows:\n",
      "$$ p(x_i | \\sigma) = \\mathcal{N}(x_i ;\\; 0, \\sigma^2) $$\n",
      "$$ p(\\sigma) = \\mathrm{Exponential}(\\sigma ;\\; 1) = e^{-\\sigma}. $$ \n",
      "\n",
      "In order to apply standard MH sampling with an unbounded proposal, we will apply a transformation to parametrise the model using $y = \\log(\\sigma)$. As $\\sigma \\in \\mathbb{R}^+$, $y \\in \\mathbb{R}$.\n",
      "\n",
      "We can derive the distribution over $y$ using the change of variables formula (see Sec. 2.10 in Computational Statistics by Koistinen in Moodle):\n",
      "$$ f_Y(y) = f_\\sigma(\\sigma) \\left| \\frac{\\mathrm{d}\\sigma}{\\mathrm{d}y} \\right| $$\n",
      "\n",
      "1. Express and plot the prior density $p(\\sigma)$ as a function of $y = \\log \\sigma$.\n",
      "2. Check that your density is valid by evaluating $\\int_{-\\infty}^{\\infty} f_Y(y) \\mathrm{d}y$.\n",
      "3. Generate a data set of 10 points $x_i$ with $x_i \\sim \\mathcal{N}(0, 2^2)$ (zero-mean normal with variance 2^2).\n",
      "4. Implment a MH sampler to sample $y = \\log \\sigma$ when $\\log P^*(y) = \\sum_{i=1}^{10} \\log p(x_i | y) + \\log p(y)$ using $Q(x' ; x) = \\mathcal{N}(x' ;\\; x, 1)$ as the proposal.\n",
      "5. Plot a histogram of the obtained samples for $\\sigma$. (Hint: remember to invert the transformation used to obtain $y$.)\n",
      "\n",
      "(Hint: If you wish, you can use autograd to evaluate the derivative needed in the transformation, although in this case it is easy to evaluate by hand as well.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. Metropolis-Hastings sampling of a higher dimensional target\n",
      "\n",
      "1. Implement a Metropolis-Hastings sampler for multivariate targets and test it for sampling from standard zero-mean unit covariance $d$-dimensional multivariate normal $\\mathcal{N}(0, I_d)$ for $d = 2$ using $Q(x' ; x) = \\mathcal{N}(x' ;\\; x, w I_d)$ as the proposal with $w = 0.5$. Draw 10000 samples and thin by a factor of 10.\n",
      "2. Plot trace plots, histograms and pairwise scatter plots of all pairs of variables.\n",
      "3. Run the sampler and draw the plots again with different $w$. How do the results change? Can you see the link between the acceptance ratio and the appearence of the plots?\n",
      "4. Repeat the analysis for $d=5$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 4. Gibbs sampling\n",
      "\n",
      "Gibbs sampling is a popular special form of multivariate Metropolis-Hastings sampling where a vector of variables $x = (x_1, \\dots, x_n)$ is updated cyclically one at a time $x_1, x_2, \\dots, x_n, x_1, x_2, \\dots$. For each update, $x_i$ is drawn from the exact conditional distribution $p(x_i | x_{\\setminus i})$ given the other variables $x_{\\setminus i}$.\n",
      "\n",
      "Implement the Gibbs sampler to draw samples from a 2-dimensional multivariate normal with covariance $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ as described at https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/\n",
      "\n",
      "Plot the trace plots and normed histograms of your samples. Test your sampler with different values of $\\rho$. What do you observe?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}