{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 11: MCMC and gradients 1: Hamiltonian Monte Carlo basics and Metropolis-adjusted Langevin algorithm\n",
      "\n",
      "In this lecture we will consider using gradient information of the target function to improve MCMC convergence. These methods are especially useful in high dimensions where a simple random walk has many more directions to wonder off and tracing a narrow high probability density shell will be very difficult.\n",
      "\n",
      "## Metropolis-adjusted Langevin algorithm\n",
      "\n",
      "The simplest method for doing this is the Metropolis-adjusted Langevin algorithm (MALA) which simulates a reversible Langevin diffusion whose stationary distribution is the target $P(x)$:\n",
      "$$ \\mathrm{d}x_t = \\sigma \\mathrm{d}B_t + \\frac{\\sigma^2}{2} \\nabla \\log P(x_t) \\mathrm{d}t. $$\n",
      "This expression is a *stochastic differential equation*, where $B_t$ denotes the standard $d$-dimensional Brownian motion (a stochastic process with independent normally distributed increments).\n",
      "\n",
      "Perhaps the simplest way to understand the Langevin diffusion is through its time discretisation:\n",
      "$$ x'_{t+1} = x_t + \\sigma_n z_{t+1} + \\frac{\\sigma_n^2}{2} \\nabla \\log P(x_t), $$\n",
      "where $z_{t+1} \\sim \\mathcal{N}(0, I_d).$\n",
      "\n",
      "With decreasing $\\sigma_n$ the discretisation gives better approximations of the original diffusion, but convergence to the correct stationary distribution will require the addition of the Metropolis-Hastings accept/reject step, where the proposal is accepted as usual with probability\n",
      "$$ \\alpha(x_t, x'_{t+1}) = \\frac{P(x'_{t+1})}{P(x_t)} \\frac{q(x_t; x'_{t+1})}{q(x'_{t+1}; x_t)}, $$\n",
      "where\n",
      "$$ q(x'_{t+1}; x_t) = \\mathcal{N}\\left(x'_{t+1};\\; x_t + \\frac{\\sigma_n^2}{2} \\nabla \\log P(x_t), \\sigma_n^2 I_d \\right). $$\n",
      "\n",
      "MALA helps in finding a good direction which is important especially in high-dimensional spaces, but it still needs to take relatively short steps to achieve a high acceptance rate.\n",
      "\n",
      "## Hamiltonian Monte Carlo (HMC)\n",
      "\n",
      "An even more efficient method can be developed through defining a dynamical system where $- \\log P(x)$ represents the potential energy of the system which is then simulated using Hamiltonian dynamics. Hamiltonian dynamics preserve the energy and volume of the system and by using a suitable (symplectic) numeric solver, the same applies even to the numerical solution. This allows simulating very long trajectories with a high acceptance probability.\n",
      "\n",
      "For HMC we augment each regular \"position\" variable $x_i$ with a corresponding \"momentum\" variable $r_i \\sim \\mathcal{N}(0, 1)$ (denoted by $p_i$ in many of the more physical references). Denoting the potential energy defined by the target as $U(x) = -\\log P(x)$, the momentum variables have a corresponding kinetic energy $K(r) = \\sum_{i=1}^d  \\frac{1}{2} r_i^2$ with the total energy given by $H(x, r) = U(x) + K(r)$.\n",
      "\n",
      "Hamiltonian dynamics of the system are defined by\n",
      "$$ \\frac{\\mathrm{d} x_i}{\\mathrm{d}t} = \\frac{\\partial H(x, r)}{\\partial r_i} \\\\\n",
      "\\frac{\\mathrm{d} r_i}{\\mathrm{d}t} = - \\frac{\\partial H(x, r)}{\\partial x_i} $$\n",
      "\n",
      "For our choice of $H(x, r)$ these become\n",
      "$$ \\frac{\\mathrm{d} x_i}{\\mathrm{d}t} = r_i \\\\\n",
      "\\frac{\\mathrm{d} r_i}{\\mathrm{d}t} = - \\frac{\\partial U(x)}{\\partial x_i} = \\frac{\\partial \\log P(x)}{\\partial x_i}$$\n",
      "\n",
      "The most common approach to simulate these numerically is to use the so-called leapfrog integrator which is specifically designed for Hamiltonian systems:\n",
      "$$ r_i(t + \\epsilon/2) = r_i(t) - (\\epsilon/2) \\frac{\\partial U(x(t))}{\\partial x_i} \\\\\n",
      "  x_i(t + \\epsilon) = x_i(t) + \\epsilon r_i(t + \\epsilon/2) \\\\\n",
      "  r_i(t + \\epsilon) = r_i(t) - (\\epsilon/2) \\frac{\\partial U(x(t + \\epsilon))}{\\partial x_i}\n",
      "$$\n",
      "Compared to standard numerical differential equation solvers, the leapfrog integrator allows for stable solutions over a much longer time scale because the total energy of the system is conserved.\n",
      "\n",
      "For more details, please see the tutorials linked below.\n",
      "\n",
      "Note: the HMC algorithm presented here can be extended to include \"masses\" for the \"particles\", as suggested by Neal (see references below). To simplify the notation we assume all \"particles\" have unit mass which may not be optimal for all applications.\n",
      "\n",
      "## Test cases\n",
      "\n",
      "Throughout the lecture, we will consider the following examples as test cases:\n",
      "1. Circle: $ \\log p_1(x) = -20*(\\sqrt{x_1^2+x_2^2}-10)^2 $\n",
      "2. Correlated 2-d normal: $ p_2(x) = \\mathcal{N}(x ;\\; 0, \\Sigma_2) $, where $ \\Sigma_2 = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $\\rho = 0.998$\n",
      "3. 20-d normal: $ p_3(x) = \\mathcal{N}(x ;\\; 0, \\Sigma_3) $, where $\\Sigma_3 = \\mathrm{diag}(0.05^2, 0.1^2, \\dots, 0.95^2, 1.00^2)$.\n",
      "\n",
      "Note: Assuming proposals use $\\sigma^2 I_d$ covariance, all considered algorithms are rotation invariant. Example 3 thus represents any high-dimensional normal with this particular range of scales for different directions.\n",
      "\n",
      "To assess the quality of the approximation for $p_3$, it is a good idea to plot the means and standard deviations of the samples and compare those with the ground truth.\n",
      "\n",
      "## Further reading:\n",
      "\n",
      "David J. C. MacKay: Information Theory, Inference, and Learning Algorithms (Chapter 30)  \n",
      "http://www.inference.org.uk/itila/book.html  \n",
      "http://www.inference.org.uk/mackay/itprnn/ps/387.412.pdf  \n",
      "(Very brief introduction to HMC for the impatient)\n",
      "\n",
      "Radford M. Neal: MCMC using Hamiltonian Dynamics  \n",
      "https://arxiv.org/pdf/1206.1901.pdf  \n",
      "(A classical and very comprehensive HMC tutorial)\n",
      "\n",
      "Michael Betancourt: A Conceptual Introduction to Hamiltonian Monte Carlo  \n",
      "https://arxiv.org/pdf/1701.02434.pdf  \n",
      "(A more modern HMC tutorial)\n",
      "\n",
      "Roberts and Rosenthal: Optimal scaling of discrete approximations to Langevin diffusions. J RSS B 60(1):255-268 (1998).  \n",
      "https://doi.org/10.1111/1467-9868.00123  \n",
      "(Canonical MALA reference)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. MCMC videos (Optional)\n",
      "\n",
      "If you can make this work on your computer, creating videos of MCMC method operation can be very useful to see how they perform.\n",
      "\n",
      "Try out the code below if you can make it work to visualise the performance of different MCMC algorithms by plotting a sequence of samples overlaid over a contour plot of the target. If you do not have the necessary software to make it work on your laptop, you can try with the computer lab Linux computers or just skip this.\n",
      "\n",
      "Test it with standard Metropolis-Hastings sampling of the example target distributions shown above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hint: many latter examples will use autograd so we will use autograd.numpy libraries throughout\n",
      "%matplotlib inline\n",
      "import autograd.numpy as np\n",
      "import autograd.numpy.random as npr\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def d_mhsample(x0, n, target, proposal, drawproposal, return_accrate=False):\n",
      "    x = x0\n",
      "    d = len(x0)\n",
      "    accepts = 0\n",
      "    lp = target(x)\n",
      "    xs = np.zeros([n, d])\n",
      "    for i in range(n):\n",
      "        x_prop = drawproposal(x)\n",
      "        l_prop = target(x_prop)\n",
      "        a = l_prop - lp + proposal(x_prop, x) - proposal(x, x_prop)\n",
      "        if  0<a or npr.rand() < np.exp(a):\n",
      "            x = x_prop\n",
      "            lp = l_prop\n",
      "            accepts += 1\n",
      "        xs[i] = x\n",
      "    if return_accrate:\n",
      "        return accepts/n\n",
      "    else:\n",
      "        print('Acceptance rate:', accepts/n)\n",
      "        return xs[len(xs)//2:]"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import autograd.numpy.linalg as npl\n",
      "\n",
      "def circle(x):\n",
      "    return -20*(np.sqrt(x[0]**2+x[1]**2)-10)**2\n",
      "\n",
      "samples1 = d_mhsample(np.zeros(2), 30000, circle, lambda x,y: 0, lambda x: x+npr.normal(size=2))\n",
      "plt.plot(samples1[:,0], samples1[:,1], '.')\n",
      "print(np.mean(samples1, 0), np.std(samples1, 0))"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "from matplotlib import animation, rc\n",
      "\n",
      "samples = samples1\n",
      "\n",
      "# First set up the figure, the axis, and the plot element we want to animate\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "ax.set_xlim((-12, 12))\n",
      "ax.set_ylim((-12, 12))\n",
      "\n",
      "x = np.linspace(-12.0, 12.0, 100)\n",
      "y = np.linspace(-12.0, 12.0, 100)\n",
      "X, Y = np.meshgrid(x, y)\n",
      "\n",
      "Z = np.zeros((len(x), len(y)))\n",
      "# Need to use a for loop here because f() does not support vectorised evaluation\n",
      "for i, x_i in enumerate(x):\n",
      "    for j, y_j in enumerate(y):\n",
      "        Z[i,j] = circle(np.array([x_i,y_j]))\n",
      "\n",
      "plt.contour(X, Y, np.log(np.abs(Z.T)))\n",
      "pts, = ax.plot([], [], 'r', lw=2)\n",
      "\n",
      "# initialization function: plot the background of each frame\n",
      "def init():\n",
      "    pts.set_data([], [])\n",
      "    return (pts,)\n",
      "\n",
      "# animation function. This is called sequentially\n",
      "def animate(i):\n",
      "    pts.set_data(samples[np.max([0, 10*i-500]):10*i:5,0], samples[np.max([0, 10*i-500]):10*i:5,1])\n",
      "    return (pts,)\n",
      "\n",
      "# call the animator. blit=True means only re-draw the parts that have changed.\n",
      "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
      "                               frames=1000, interval=20, blit=True)\n",
      "\n",
      "HTML(anim.to_html5_video())"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. HMC sampling\n",
      "\n",
      "Implement a HMC sampler and test it on the example models given above. You can use `autograd` to evaluate the gradients of the target distributions.\n",
      "\n",
      "As a starting point, here is a Python translation for a simple Hamiltonian Monte Carlo algorithm from Chapter 30 of MacKay's Information Theory, Inference, and Learning Algorithms (http://www.inference.org.uk/mackay/itprnn/ps/387.412.pdf), edited to use log-probabilities (higher is preferred) instead of energies (lower is preferred) as in the original:\n",
      "\n",
      "``` {python}\n",
      "g = gradF(x)  # set gradient using initial x\n",
      "logP = target(x) # set log-target function too\n",
      "for m in range(M): # collect M samples\n",
      "    p = npr.normal(size=x.shape)  # initial momentum is Normal(0,1)\n",
      "    H = p.T @ p / 2 - logP   # evaluate H(x,p)\n",
      "    xnew = np.copy(x)\n",
      "    gnew = np.copy(g)\n",
      "    for l in range(L): # make L \u2018leapfrog\u2019 steps\n",
      "        p = p + epsilon * gnew / 2   # make half-step in p\n",
      "        xnew = xnew + epsilon * p    # make step in x\n",
      "        gnew = gradF(xnew)           # find new gradient\n",
      "        p = p + epsilon * gnew / 2   # make half-step in p\n",
      "    logPnew = target(xnew)           # find new value of H\n",
      "    # in theory we should now set p = -p to make the chain reversible,\n",
      "    # but as we are not storing p values this makes no difference\n",
      "    Hnew = p.T @ p / 2 - logPnew\n",
      "    dH = Hnew - H    # Decide whether to accept, H = - log P, so need to flip sign in MH test\n",
      "    if dH < 0 or np.log(npr.rand()) < -dH:\n",
      "        g = gnew\n",
      "        x = xnew\n",
      "        logP = logPnew\n",
      "```\n",
      "\n",
      "Note: The sampler depends on the number of integration steps $L$ and step length $\\epsilon$. We will consider their tuning in more detail tomorrow. For now, you can manually find a small enough $\\epsilon$ to get a reasonable acceptance rate (around 60% is theoretically optimal in certain conditions), trying different values of $L$.\n",
      "\n",
      "You can try things yourself. Assuming your sampler is correctly implemented, increasing $\\epsilon$ typically leads to a gradual drop in the acceptance until at some point there is a catastrophic drop to 0.\n",
      "\n",
      "HMC sampler is different from other MCMC samplers in that you typically need significantly fewer samples. 1000 is usually plenty and you can start with as few as 100. A critical quantity here is $\\epsilon L$ which tells the effective length of the trajectories. Small $\\epsilon L$ usually means more samples are needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. MALA sampling\n",
      "\n",
      "Implement a MALA sampler for the same examples as above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}
