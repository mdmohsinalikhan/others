{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:b2cdfb776e28f8cfef9736701104437bdb8a8f1b9b73c1d12d76afa62fb81aa7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Week 2 exercises"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Useful imports.\n",
      "# The exercises make use of autograd - see Thursday's exercises for installation instructions and more help.\n",
      "\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import autograd.numpy as np\n",
      "import numpy.random as npr\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.linalg import cholesky, cho_factor, cho_solve\n",
      "import scipy.integrate"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. Evaluating probabilities by numerical integration\n",
      "\n",
      "Consider a two-dimensional multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ with zero mean and covariance $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$.\n",
      "\n",
      "1. Implement a function that computes the pdf $p(x) = \\mathcal{N}(x;\\; \\mu, \\Sigma)$.\n",
      "2. As a sanity check, use numerical quadrature to evaluate the probability $p(-\\infty < x_1, x_2 < \\infty) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty p(x) \\mathrm{d}x_1 \\mathrm{d}x_2$ when $\\rho = 0.5$.\n",
      "3. Use numerical quadrature to evaluate the probability $p(x_1 > 0, x_2 > 0) = \\int_0^\\infty \\int_0^\\infty p(x) \\mathrm{d}x_1 \\mathrm{d}x_2$ when $\\rho = 0.5$.\n",
      "4. Evaluate the same probability when $\\rho = 0.9$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.integrate\n",
      "\n",
      "rho = 0.00\n",
      "\n",
      "def p(x1,x2):\n",
      "    d = 2\n",
      "    mu = np.array([[0, 0]]).transpose()\n",
      "    x = np.array([[x1, x2]]).transpose()\n",
      "    S = np.array([[1.0, rho], [rho, 1.0]])\n",
      "    A = np.linalg.cholesky(S)\n",
      "\n",
      "    log_det_S = 2*np.log(np.linalg.det(A))\n",
      "\n",
      "    y = np.linalg.solve(A, x - mu)\n",
      "    third_term = .5*np.dot(y.transpose(),y)\n",
      "\n",
      "    log_density = -.5*d*np.log(2*np.pi) - .5*log_det_S - third_term\n",
      "\n",
      "    return np.exp(log_density)\n",
      "\n",
      "#x = np.array([[.5, .5]]).transpose()\n",
      "#print(p(0,0))\n",
      "\n",
      "rho = .5\n",
      "print(\"integration from minus infinity to plus infinity for both variables(rho = .5):\")\n",
      "result = scipy.integrate.dblquad(p,-np.inf,np.inf,lambda x:-np.inf,lambda x:np.inf)\n",
      "print(result)\n",
      "print(\"integration from zero to plus infinity for both variables (rho = .5):\")\n",
      "result = scipy.integrate.dblquad(p,0,np.inf,lambda x:0,lambda x:np.inf)\n",
      "print(result)\n",
      "\n",
      "rho = .9\n",
      "print(\"integration from minus infinity to plus infinity for both variables(rho = .9):\")\n",
      "result = scipy.integrate.dblquad(p,-np.inf,np.inf,lambda x:-np.inf,lambda x:np.inf)\n",
      "print(result)\n",
      "print(\"integration from zero to plus infinity for both variables (rho = .9):\")\n",
      "result = scipy.integrate.dblquad(p,0,np.inf,lambda x:0,lambda x:np.inf)\n",
      "print(result)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "integration from minus infinity to plus infinity for both variables(rho = .5):\n",
        "(1.0000000000098548, 1.0202432855222756e-08)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "integration from zero to plus infinity for both variables (rho = .5):\n",
        "(0.33333333333826065, 7.0842503680296144e-09)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "integration from minus infinity to plus infinity for both variables(rho = .9):\n",
        "(1.0000000011369996, 9.568724456125979e-09)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "integration from zero to plus infinity for both variables (rho = .9):\n",
        "(0.42821685460337705, 9.109433323018835e-09)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. Gradients of a multivariate normal likelihood\n",
      "\n",
      "We will test the evaluation and gradient of the multivariate normal likelihood using a covariance matrix specified by a Gaussian process. A 1D Gaussian process is a distribution over continuous functions such that the joint distribution over any finite set of points is multivariate normal (Gaussian). We will use the so-called squared exponential covariance function over input points $t_1, \\dots, t_n$ defined by  \n",
      "$$ k(t_i, t_j) = \\exp\\left( -\\frac{(t_i-t_j)^2}{2 l^2} \\right), $$\n",
      "where the parameter $l$ is called the length scale of the process.\n",
      "\n",
      "1. Write a function to generate a $3 \\times 3$ covariance matrix $K$ such that $K_{i,j} = k(t_i, t_j)$ where $t_1 = 1, t_2 = 2, t_3 = 3$ as a function of $l$.\n",
      "2. Evaluate the log-pdf of the vector $x = (1, 0, 1)$ under a multivariate normal with 0 mean and the above covariance $K$ and $l = 1$ and report it to Moodle.\n",
      "3. Evaluate the finite difference approximation to the gradient of the log-pdf with respect to $l$ at the point $x$ with $h = 10^{-3}$ (1e-3) and report it to Moodle.\n",
      "4. Evaluate the exact gradient of the log-pdf with respect to $l$ at the point $x$ and report it to Moodle."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import autograd.numpy as np\n",
      "import autograd.numpy.linalg as lalg\n",
      "import autograd\n",
      "\n",
      "# Your code here\n",
      "def covariance(l):\n",
      "    k_1_1 = 1.0\n",
      "    k_1_2 = np.exp(-((1-2)**2)/(2.0*(l**2)))\n",
      "    k_1_3 = np.exp(-((1-3)**2)/(2.0*(l**2)))\n",
      "    \n",
      "    k_2_1 = np.exp(-((2-1)**2)/(2.0*(l**2)))\n",
      "    k_2_2 = 1.0\n",
      "    k_2_3 = np.exp(-((2-3)**2)/(2.0*(l**2)))\n",
      "    \n",
      "    k_3_1 = np.exp(-((3-1)**2)/(2.0*(l**2)))\n",
      "    k_3_2 = np.exp(-((3-2)**2)/(2.0*(l**2))) \n",
      "    k_3_3 = 1.0\n",
      "    \n",
      "    K = np.array([[k_1_1, k_1_2, k_1_3], [k_2_1, k_2_2, k_2_3], [k_3_1, k_3_2, k_3_3]])\n",
      "    \n",
      "    return K\n",
      "\n",
      "def log_pdf(l):\n",
      "    mu = np.array([[0, 0, 0]]).transpose()\n",
      "    S = covariance(l)\n",
      "    x = np.array([[1,0,1]]).transpose()\n",
      "    d=3\n",
      "    A = np.linalg.cholesky(S)\n",
      "    log_det_S = 2*np.log(np.linalg.det(A))\n",
      "    y = np.linalg.solve(A, x - mu)\n",
      "    third_term = .5*np.dot(y.transpose(),y)\n",
      "    log_density = -.5*d*np.log(2*np.pi) - .5*log_det_S - third_term\n",
      "    return log_density\n",
      "\n",
      "h = 10**(-3)\n",
      "print(\"log pdf evaluated at (1,0,1), with the above covariance S and l = 1:\")\n",
      "print(log_pdf(1))\n",
      "\n",
      "print(\"finite difference calculated at (1,0,1) with the covariance S and l = 1:\")\n",
      "print((log_pdf(1+h)-log_pdf(1))/h)\n",
      "\n",
      "\n",
      "autograd.grad(log_pdf)(1.0)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "log pdf evaluated at (1,0,1), with the above covariance S and l = 1:\n",
        "[[-4.72808403]]\n",
        "finite difference calculated at (1,0,1) with the covariance S and l = 1:\n",
        "[[-4.3558613]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "array(-4.348948023427575)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true,
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. Student-t regression\n",
      "\n",
      "In this task we will apply linear regression to data from the Framingham Heart Study that studies the association between heart disease and its causes.\n",
      "\n",
      "A description of the data and its fields can be found at http://www.maths.utas.edu.au/DHStat/Data/Flow.html\n",
      "\n",
      "Unlike in the exercise during the lectures, we will use the Student-t likelihood for the noise. This provides a form of robust regression.\n",
      "\n",
      "1. Load the data using the below code and plot the data.\n",
      "2. Fit the data using standard linear regression with normally distributed errors. Report the $\\alpha$ and $\\beta$ you found in Moodle.\n",
      "3. Implement linear regression using the Student-t log-likelihood function with $\\nu = 5$ and maximise it (i.e. minimise the negative log-likelihood) using `scipy.optimize.minimize()` and `autograd.grad()`. Plot the fitted regression line in the same figure as the data.\n",
      "4. Compare the coefficients $\\alpha$ and $\\beta$ you found for Student-t regression with those from the standard linear regression. What can you observe? Report the coefficients you found in Moodle.\n",
      "\n",
      "Hint: feel free to use the Student-t log-likelihood function provided below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import autograd.numpy as np\n",
      "import autograd.scipy as scipy\n",
      "import autograd\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import minimize\n",
      "\n",
      "# load the data from CSV file using pandas\n",
      "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
      "# convert the variables of interest to numpy arrays for autograd compatibility\n",
      "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group\n",
      "x = np.array(fram['CHOL'])\n",
      "# target: Systolic blood pressure, examination 1\n",
      "y = np.array(fram['SBP'])\n",
      "\n",
      "def linear_regression_logl(coefs, x, y):\n",
      "    return np.sum(-0.5*np.log(2*np.pi)-0.5*(y - coefs[1] - coefs[0]*x)**2)\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    myfun = lambda c: -linear_regression_logl(c, np.array(x), np.array(y))\n",
      "    dmyfun = autograd.grad(myfun)\n",
      "    v = minimize(myfun, np.ones(2), jac=dmyfun)\n",
      "    return v\n",
      "\n",
      "def linear_regression_student(x, y):\n",
      "    myfun = lambda c: -linear_regression_logl_student(c, np.array(x), np.array(y))\n",
      "    dmyfun = autograd.grad(myfun)\n",
      "    v = minimize(myfun, np.ones(2), jac=dmyfun)\n",
      "    return v\n",
      "\n",
      "# A function implementing the log-pdf of the Student-t distribution\n",
      "def student_logpdf(x, nu):\n",
      "    return scipy.special.gammaln(0.5*(nu+1)) - scipy.special.gammaln(0.5*nu) - 0.5*np.log(nu*np.pi) - 0.5*(nu+1)*np.log(1 + x**2/nu)\n",
      "\n",
      "def linear_regression_logl_student(coefs, x, y):\n",
      "    return np.sum(student_logpdf((y - coefs[1] - coefs[0]*x),5))\n",
      "\n",
      "\n",
      "\n",
      "v_lin = linear_regression(x, y)\n",
      "print(v_lin)\n",
      "\n",
      "v_lin = linear_regression_student(x, y)\n",
      "print(v_lin)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      fun: 540943.8951940064\n",
        " hess_inv: array([[  3.34822935e-07,  -7.85644964e-05],\n",
        "       [ -7.85644964e-05,   1.91521190e-02]])\n",
        "      jac: array([-0.02574829, -0.00010562])\n",
        "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
        "     nfev: 47\n",
        "      nit: 2\n",
        "     njev: 35\n",
        "   status: 2\n",
        "  success: False\n",
        "        x: array([  6.96962768e-02,   1.31732207e+02])\n",
        "      fun: 16674.11029042084\n",
        " hess_inv: array([[  1.13826385e-05,  -2.51595158e-03],\n",
        "       [ -2.51595158e-03,   5.70736295e-01]])\n",
        "      jac: array([  6.66939377e-06,   3.31588250e-08])\n",
        "  message: 'Optimization terminated successfully.'\n",
        "     nfev: 28\n",
        "      nit: 13\n",
        "     njev: 28\n",
        "   status: 0\n",
        "  success: True\n",
        "        x: array([  8.18577159e-03,   1.37931460e+02])\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Maximum likelihood estimation of the gamma distribution\n",
      "\n",
      "In this exercise we test maximum likelihood estimation of the gamma distribution to a given data set. We will again use `autograd` and `scipy.optimize.minimize` for solving this task.\n",
      "\n",
      "Gamma distribution has an additional constraint that its parameters $\\alpha$ and $\\beta$ need to be positive. This can be accomplised by using $\\log \\alpha$ and $\\log \\beta$ as variables in the optimisation.\n",
      "\n",
      "1. Load the data set $\\mathcal{D} = \\{ x_i | i = 1, \\dots, n \\}$ using the function below.\n",
      "2. Find the maximum likelihood fit for the parameters $\\alpha$ and $\\beta$ of the Gamma distribution $\\Gamma(\\alpha, \\beta)$ when $x_i \\sim \\Gamma(\\alpha, \\beta)$.\n",
      "3. Plot the maximum likelihood distribution together with a normed histogram of the data to check your estimates.\n",
      "4. Report your estimates $\\alpha$ and $\\beta$ in Moodle. (Note: please remember to report $\\alpha$ and $\\beta$, not $\\log \\alpha$ and $\\log \\beta$.)\n",
      "\n",
      "Hint: feel free to use the implementation of the gamma distribution pdf provided below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import autograd.scipy.special as scs\n",
      "import autograd.numpy as np\n",
      "import pandas as pd\n",
      "from scipy.optimize import minimize\n",
      "import autograd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
      "data = np.array(data)\n",
      "\n",
      "def alpha_beta_estimation(data):\n",
      "    myfun = lambda c: -gamma_logpdf_sum(data,c)\n",
      "    dmyfun = autograd.grad(myfun)\n",
      "    v = minimize(myfun, np.ones(2), jac=dmyfun, method='L-BFGS-B')\n",
      "    return v\n",
      "\n",
      "def gamma_logpdf_sum(data,c):\n",
      "    alpha = np.exp(c[0])\n",
      "    beta = np.exp(c[1])\n",
      "    return np.sum(gamma_logpdf(data,alpha,beta))\n",
      "\n",
      "def gamma_logpdf(x, alpha, beta):\n",
      "    return (alpha*np.log(beta) - scs.gammaln(alpha) + (alpha-1) * np.log(x) - beta * x)\n",
      "\n",
      "v_lin = alpha_beta_estimation(data)\n",
      "print(v_lin)\n",
      "print(np.exp(v_lin.x[0]))\n",
      "print(np.exp(v_lin.x[1]))\n",
      "\n",
      "alpha = np.exp(v_lin.x[0])\n",
      "beta = np.exp(v_lin.x[1])\n",
      "\n",
      "t = np.linspace(0,10,100)\n",
      "plt.plot(t, np.exp(gamma_logpdf(t,alpha,beta)))\n",
      "plt.hist(data,normed = True)\n",
      "plt.show()\n",
      "\n",
      "\n",
      "#print(data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      fun: 905.28081351531205\n",
        " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
        "      jac: array([  6.26873348e-05,  -5.15570430e-05])\n",
        "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
        "     nfev: 11\n",
        "      nit: 9\n",
        "   status: 0\n",
        "  success: True\n",
        "        x: array([ 1.87106675,  0.4905797 ])\n",
        "6.4952214536\n",
        "1.63326274309\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "(array([ 0.00428082,  0.05779111,  0.18621579,  0.25898978,  0.23330484,\n",
        "         0.16481168,  0.12414386,  0.03210617,  0.00642123,  0.00214041]),\n",
        " array([  8.73300000e-03,   9.43132800e-01,   1.87753260e+00,\n",
        "          2.81193240e+00,   3.74633220e+00,   4.68073200e+00,\n",
        "          5.61513180e+00,   6.54953160e+00,   7.48393140e+00,\n",
        "          8.41833120e+00,   9.35273100e+00]),\n",
        " <a list of 10 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true,
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 5. Fitting a normal mixture model\n",
      "\n",
      "A normal mixture model, also known as Gaussian mixture model, can be described as a weighted mixture of several normal distributions.\n",
      "\n",
      "A $k$-component mixture has the pdf\n",
      "$$ p(x) = \\sum_{i=1}^k \\pi_i \\mathcal{N}(x;\\; \\mu_i, \\sigma_i), $$\n",
      "where $\\pi_i$ are the mixing weights that satisfy $0 \\le \\pi_i 1, i=1, \\dots, k$ and $\\pi_1 + \\dots + \\pi_k = 1$, and $\\mu_i$ are the means of the $k$ components and $\\sigma_i$ are the corresponding standard deviations. To simplify the problem, we only consider $k=2$ and assume $\\sigma_1 = \\sigma_2 = 1$.\n",
      "\n",
      "Your task is to use maximum likelihood estimation to fit this kind of a mixture model to the data set from the previous exercise.\n",
      "\n",
      "1. Implement the pdf of the above Gaussian mixture model.\n",
      "2. Evaluate $\\log p(1)$ under a mixture model where $p_1 = 0.2$, $\\mu_1 = 0.5$, $\\mu_2 = 2$. Report your result to Moodle.\n",
      "3. Implement maximum likelihood estimation of the parameters of the mixture. Remember to make sure your $\\pi_i$ satisfy the constraints. One way to achieve this is to use a single real valued parameter $x$ and set\n",
      "$$ \\pi_1 = \\frac{1}{1 + \\exp(-x)}, \\pi_2 = \\frac{\\exp(-x)}{1 + \\exp(-x)}. $$\n",
      "This is known as the logistic transformation.\n",
      "4. Plot the pdf of the mixture together with the normed histogram of the data.\n",
      "5. In order to fix the order between the components, choose $\\pi_1 > \\pi_2$. Report your estimates of $\\pi_1$, $\\mu_1$, $\\mu_2$.\n",
      "\n",
      "Hint: You need to be careful with choosing the initial point for the optimisation. In particular, it is very important to have $\\mu_1 \\neq \\mu_2$. If you start at a symmetric setting, the model will never be able to break the symmetry."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import autograd.scipy.special as scs\n",
      "import autograd.numpy as np\n",
      "import pandas as pd\n",
      "from scipy.optimize import minimize\n",
      "import autograd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
      "data = np.array(data)\n",
      "\n",
      "def normal_log_pdf(x,mu):\n",
      "    return -0.5*np.log(2*np.pi)-0.5*((x-mu)**2)\n",
      "\n",
      "def finding_fit(data):\n",
      "    myfun = lambda c : -mixture_logpdf_sum(data,c)\n",
      "    dmyfun = autograd.grad(myfun)\n",
      "    d = [.2,1,7]\n",
      "    v = minimize(myfun, d, jac=dmyfun, method='L-BFGS-B')\n",
      "    return v\n",
      "\n",
      "def mixture_logpdf_sum(data,c):\n",
      "    pi1 = 1/(1 + np.exp(-c[0]))\n",
      "    pi2 = np.exp(-c[0])/(1 + np.exp(-c[0]))\n",
      "    mu1 = c[1]\n",
      "    mu2 = c[2]\n",
      "    return np.sum(mixture_log_pdf(data,pi1,mu1,mu2))\n",
      "\n",
      "def mixture_log_pdf(x,pi1,mu1,mu2):\n",
      "    return np.log(pi1*np.exp(normal_log_pdf(x,mu1)) + (1-pi1)*np.exp(normal_log_pdf(x,mu2)))\n",
      "\n",
      "print(mixture_log_pdf(1,.2,.5,2))\n",
      "\n",
      "v = finding_fit(data)\n",
      "print(v)\n",
      "\n",
      "x = np.linspace(-10,10,100)\n",
      "plt.hist(data,normed = True)\n",
      "pi1 = 1/(1 + np.exp(-v.x[0]))\n",
      "mu1 = v.x[1]\n",
      "mu2 = v.x[2]\n",
      "plt.plot(x,np.exp(mixture_log_pdf(x,pi1,mu1,mu2)))\n",
      "print(\"pi1:\")\n",
      "print(pi1)\n",
      "print(\"mu1:\")\n",
      "print(mu1)\n",
      "print(\"mu2:\")\n",
      "print(mu2)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1.33184540021\n",
        "      fun: 877.91701884263341\n",
        " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
        "      jac: array([ 0.00160034, -0.0012593 , -0.00065036])\n",
        "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
        "     nfev: 10\n",
        "      nit: 9\n",
        "   status: 0\n",
        "  success: True\n",
        "        x: array([ 0.7901856 ,  3.28879165,  5.49312344])\n",
        "pi1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.687871180704\n",
        "mu1:\n",
        "3.28879165245\n",
        "mu2:\n",
        "5.49312344223\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt01PWd//HnO9zvhIvhHu4oakUQ7KpAlCK0grTWtlhr\ntVs9Vmpbu91ddX+nFdbt6dY92227rts9rfW2trRa6yLeUGkQ79xVAklISIAQwjXKRUIun98fM6FD\nSDKTZGY+8515Pc6Zw8z3Ni8wvvjy+d7MOYeIiGSGLN8BREQkeVT6IiIZRKUvIpJBVPoiIhlEpS8i\nkkFU+iIiGSSm0jez+Wa23cyKzOzuZubfbmbvm9kmM3vdzM6NmHevmRWb2TYzuzqe4UVEpG0s2nn6\nZpYFFAFzgL3AOmCxc257xDK9nXPHwu8XAkucc581s8nAk8B0YATwKjDB6eIAEREvYtnTnwEUO+fK\nnXO1wHJgUeQCjYUf1htoCL+/FljunKtzzpUBxeHtiYiIB51jWGY4sDvi8x6aKW4zWwL8HdAFuCpi\n3bcjFqsITxMREQ/idiDXOfeQc248cDfww3htV0RE4ieWPf0KYFTE5xHhaS35A/CriHVHRlvXzDTG\nLyLSDs45a8vysezprwPGm1mumXUFFgMrIhcws/ERHxcQOvBLeLnFZtbVzMYA44H3WgiuV5xe9913\nn/cM6fTSn6f+PFP11R5R9/Sdc/VmdiewitBfEg8757aZ2TJgnXNuJXCnmX0GOAUcAW4Or1tgZn8E\nCoBaQmf1aK9eRMSTWIZ3cM69BExqMu2+iPd3tbLuT4CftDegiIjEj67ITUN5eXm+I6QV/XnGl/48\n/Yp6cVZSQphp1EdEpI3MDJeAA7kiIpImVPoiIhlEpS8ikkFU+iIZyjnHD1f/kEc2PcKxU8eiryBp\nQaUvkqGeLniap7c9zbOFzzLyP0Zyy7O3UHSoKPqKEmg6e0ckAx2tOcrkhybzu+t+x8zcmVQdq+Jn\nb/+Mt/a8xdpvrPUdT2Kks3dEJCb3v34/V425ipm5MwHI6Z3Dj+f8mL1H9/LGrjc8p5NEUumLZJit\n+7fyyOZHeOAzD5wxvXNWZ/7hsn/gp2/+1FMySQaVvkgGcc5x54t38qNZPyKnd85Z82+Zcgvr967n\ng6oPPKSTZFDpi2SQ/LJ89h/fzx3T72h2fvfO3fnepd/jgbceaHa+BJ9KXySDvLjjRa4/73o6Z7V8\nr8U7LrmDF4pfoKy6LHnBJGlU+iIZ5JXSV7h63NWtLtOvez9um3ob//7WvycplSSTSl8kQ+w/vp+d\nR3YyY/hZj7g+y/cu/R6Pv/84J+tOJiGZJJNKXyRDvFb6GrNHz6ZLpy5Rlx3aZygXnHMBa8rWJCGZ\nJJNKXyRDrCpdxdyxc2Ne/poJ1/B88fMJTCQ+qPRFMoBzjldKoo/nR2osfV0tn15U+iIZYPvB7XTK\n6sSEARNiXudTOZ/iVP0pCg8VJjCZJJtKXyQDrCoJDe2YxX6bFjPjc+M/x/NFGuJJJyp9kQwQy6ma\nzblmosb1041KXyTNnao/xevlrzNnzJw2rztnzBzW7V3HRyc/SkAy8UGlL5Lm3t79NhMHTmRgz4Ft\nXrdX115cMeoKXil9JQHJxAeVvkiae7X01XYN7TTSqZvpRaUvkuberXiXy0de3u71r5lwDS8Wv0iD\na4hjKvFFpS+S5rZUbeGiIRe1e/0x2WMY0GMAG/ZuiGMq8UWlL5LGBo8Zyf4D+xnZbyRm1ubXkCGj\nAZg7di6rd672+5uRuFDpi6Sxg532QNWVgGvXq6qqHIBZubN4fdfrSc8v8RdT6ZvZfDPbbmZFZnZ3\nM/O/b2ZbzWyzmb1iZiMj5tWb2UYz22Rmz8YzvIhEMQTY1/6hnUazcmfx5q43qW+o73gm8Spq6ZtZ\nFvAgMA84H7jBzM5tsthGYJpzbgrwJ+DfIuYdd85Ndc5d7Jz7fJxyi0gscoCqjpf+4F6DGd53OJv3\nbe54JvEqlj39GUCxc67cOVcLLAcWRS7gnFvjnGu88fY7wPCI2bFf9y0i8RWnPX2AWaNm8Xq5hniC\nLpbSHw7sjvi8hzNLvalvAi9GfO5mZu+Z2VtmtqillUQkvk7WnYRs4MDkuGxv9ujZrCnX/fWDLq4H\ncs3sa8A0zhzeyXXOzQBuBH5uZmPi+Z0i0ryCAwVwGKjvFpftzcqdxdpda3W+fsC1/HTkv6oARkV8\nHhGedgYz+wxwLzArPAwEgHOuMvzrTjPLBy4GdjZdf+nSpaff5+XlkZeXF0t+EWnBln1boCp+2xvW\nZxgDegxg6/6tXJhzYfw2LDHLz88nPz+/Q9uwaA9IMLNOQCEwB6gE3gNucM5ti1jmYuApYJ5zriRi\nen/ghHPulJkNAt4EFjnntjf5DqcHNYjE110v3cUv7v8FvNWR/7fsjIeo3LriVqYMmcKdM+7seEDp\nMDPDOdem46ZRh3ecc/XAncAqYCuw3Dm3zcyWmdmC8GIPAL2Ap5qcmnkesN7MNgGvAT9pWvgikhhb\nquK7pw+hIR6N6wdb1D39pITQnr5IXDnnGPjAQI7cfwSOx29Pv7y6nBm/mcG+H+xr0wNZJDESsqcv\nIsFTcbSCrp26wvH4bje3fy49OvfQIxQDTKUvkoa27OvYTdZaMyt3FmvKNMQTVCp9kTS0pWoLF+Uk\npvRn587WfXgCTKUvkoYSWfozc2eytnxtQrYtiafSF0lDiRzemTBgAjX1NZRXlydk+5JYKn2RNHOy\n7iTlH5UzaeCkhGzfzJg5aiZrd2lvP4hU+iJppuRwCaP7j6ZLpy4J+46ZozTEE1QqfZE0U3SoiIkD\nJyb0O2bmak8/qGK5946IBEjRoSImDohX6Xdr/iIsA+4G62Vwovk1c3Jy2bevLE45JF60py+SZuK7\np19Ds49SdA72zINRf25+fsSjFiW1qPRF0kzR4cQP7wBQPhNGaYgnaFT6ImkmGWP6AOyaCbkq/aBR\n6YukkeqT1ZyoPcGQ3kMS/2UVM2BwAXQ9lvjvkrhR6YukkeJDxUwcODE5d8Cs6w77psCIdxL/XRI3\nKn2RNFJ4qDA5QzuNNK4fOCp9kTQS39M1Y6Bx/cBR6YukkaQdxG20+zIYtg46nUred0qHqPRF0kjS\nS/9kfzg0MVT8EggqfZE04ZxLfukD7LwKxqxO7ndKu6n0RdJE5bFKenftTb/u/ZL7xTvnwNjXkvud\n0m4qfZE04WUvH2DXFTBsPXRp4SY8klJU+iJpwlvpn+odOl9/5FvJ/25pM5W+SJrwVvqgcf0AUemL\npAmVvsRCpS+SJryW/p5Pw+Ct0O0jP98vMVPpi6SBuoY6yqrLGJc9zlOA7lBxKeS+7uf7JWYqfZE0\nUFZdxrA+w+jWuZu/EKVzNMQTACp9kTTgdWinkcb1A0GlL5IGig8VM37AeL8hKqdB/3LoecBvDmlV\nTKVvZvPNbLuZFZnZ3c3M/76ZbTWzzWb2ipmNjJh3c3i9QjP7ejzDi0hIyZESf+P5jRo6h261POYv\nfnNIq6KWvpllAQ8C84DzgRvM7Nwmi20EpjnnpgB/Av4tvG428CNgOnApcJ+ZJfkacZH0V3KkhHED\nPJc+QMnVMP5F3ymkFbHs6c8Aip1z5c65WmA5sChyAefcGufcyfDHd4Dh4ffzgFXOuY+cc9XAKmB+\nfKKLSKPSI6X+9/QBihbCxOfB6n0nkRbEUvrDgd0Rn/fw11JvzjeBxr/qm65bEWVdEWmjBtdAWXUZ\nY7LH+I4C1aPh2BAY8a7vJNKCzvHcmJl9DZgGzG7rukuXLj39Pi8vj7y8vLjlEklnFR9XkN09m55d\nevqOElK4ECY+d+bunsRFfn4++fn5HdpGLKVfAYyK+DwiPO0MZvYZ4F5gVngYqHHdvCbrNnuUJ7L0\nRSR2pUdKU2M8v1HRQrj2VtDdluOu6Q7xsmXL2ryNWIZ31gHjzSzXzLoCi4EVkQuY2cXAr4BrnXOH\nIma9DMw1s37hg7pzw9NEJE5KjpQwNnus7xh/VTEjdNpmtu8g0pyope+cqwfuJHQQdiuw3Dm3zcyW\nmdmC8GIPAL2Ap8xsk5k9G173CHA/sB54F1gWPqArInFScjgFTteM5LKgaAF4vlZMmhfTmL5z7iVg\nUpNp90W8n9vKuo8Cj7YvnohEU1pdyoIJC6IvmExFC2HGb32nkGboilyRgCs5nGLDOwAlc2E4fHRS\nd91MNSp9kYBLmQuzItX2gl3wcokO4aUalb5IgFWfrOZU/SkG9xzsO8rZCmFF4Yroy0lSqfRFAqz0\nSCljs8diZr6jnK0QXih+gU9qP/GdRCKo9EUCLOXO3Il0FKYNm8ZzRc/5TiIR4npFrojE15Aho6mq\nKm95gcuBXmCLU3BPH7jpUzfxxPtP8OXzv+w7ioRpT18khYUK37X8GnAbHH6olWX8uu6861hbvpYD\nx3WP/VSh0hcJsuwSOJKiwztA7669WTBxAcs/XO47ioSp9EWCLLsUjqTYOfpN3PSpm3j8/cd9x5Aw\nlb5IUHU6BX32QnWu7yStmjN2DhUfV7D94HbfUQSVvkhw9SuHo8OhoYvvJK3qnNWZr174VZ7Y8oTv\nKIJKXyS4BpSk/NBOo5s+dRP/+8H/0uAafEfJeCp9kaDKLoXDqXsQN9JFQy6if/f+rN652neUjKfS\nFwmqFD9zp6kllyzhwfce9B0j46n0RYIqQMM7AF/71Nd4Y9cb7Dyy03eUjKbSFwmqAJyuGalX117c\nMuUWHlr3kO8oGU2lLxJILlBj+o2WTF/CI5sf4UTtCd9RMpZKXySIeu2Huu5Q0893kjYZmz2Wy0Ze\nxpPvP+k7SsZS6YsEUcCGdiJ999Lv8sv3folz/u8NlIlU+iJBNKAkcEM7jeaMmUN9Qz1rytf4jpKR\nVPoiQRTgPX0z4zszvsPP3/m57ygZSffTFwmi7BIon+07RRTdWn6iVxfgLrA7DQ41v0hOTi779pUl\nKlzG0p6+SBAFYk+/hhbv81/rYN2P4G9ub3GZVh8eI+2m0hcJogCP6Z+27ttw/h9CZyJJ0qj0RYKm\nywnocRiODvOdpGOOnwNbvwzT/8t3koyi0hcJmv47oXo0uE6+k3Tc238H0/879BeZJIVKXyRoAjGe\nH6NDk2D3ZXDRY76TZAyVvkjQpMN4fqS3/h4u+3ewet9JMkJMpW9m881su5kVmdndzcyfaWYbzKzW\nzK5rMq/ezDaa2SYzezZewUUyVjrt6QPsuhw+GQATXvSdJCNELX0zywIeBOYB5wM3mNm5TRYrB24G\nmruhxnHn3FTn3MXOuc93NLBIxgvYffSjM3j3u3DpL30HyQix7OnPAIqdc+XOuVpgObAocgHn3C7n\n3IeETrBtqoWrM0SkXdJtTx9g65fgnA9gcIHvJGkvltIfDuyO+LwnPC1W3czsPTN7y8wWRV9cRFpk\nDdC/DI6M8Z0kvuq7wYbbYYaerJVoybgNQ65zrtLMxgCrzex959xZj85ZunTp6fd5eXnk5eUlIZpI\nwPTZCyf7Q20v30nib/3t8O3J8NqP4WS27zQpKT8/n/z8/A5tw6Ld3tTMPg0sdc7ND3++B3DOuZ82\ns+wjwHPOuWda2Faz883M6TarImcL3bsm4v+N3DUw55/gt2/GugWaH3WNOUEH1m/Hutd9DSovhrd/\nAJhuvxyFmeGca9MQeizDO+uA8WaWa2ZdgcXAitZyRATqH14HMxsEXAZo0E6kvbJL0+wgbhPvfjc0\nxKPTNxMmauk75+qBO4FVwFZguXNum5ktM7MFAGZ2iZntBq4HfmVmH4RXPw9Yb2abgNeAnzjntifi\nNyKSEdLxIG6kihlwPAcmPu87SdqKOryTlBAa3hFp1lnDO1/8KhR/Ft6/KdYtEKjhHYCLfwuTVsDy\n/9PwThSJGt4RkVSR7nv6EDp9c3Q+pOGx6lSg0hcJkrS7MKsZp/rA9kVwoe8g6UmlLxIU3T4O3Y3y\nWI7vJIm3+RtwMRreSQCVvkhQZJeEh3Yy4CL38lnQFTZWbvSdJO2o9EWCYmAxHJ7gO0VyuCzYDI9s\nfsR3krSj0hcJigE74FCGlD7AFlj+4XJq6mp8J0krKn2RoBhQDIfH+06RPNVw0ZCLWFHY2rWg0lYq\nfZGgyKThnbBvTPkGj23RU7XiSaUvEhQDijNreAdYNGkRa3etpfpkte8oaUOlLxIE3T6Grsfg6DDf\nSZKqT7c+XDn6Sp4rfM53lLSh0hcJgtPj+RlwumYT10++nqcKnvIdI22o9EWCYMCOjBvPb7Rw4kLy\ny/L5uOZj31HSgkpfJAgGZtiZOxH6de/H7NGzWVm00neUtKDSFwmCDDyIG+n6867n6YKnfcdIC8l4\nXKKIdNTAYtj0Td8pkqxb+NbSQHfgLrCvG5yKvmZOTi779pUlMlxgaU9fJAgyck+/htD9+B2cdLB7\nPkz4w1+ntfKqqir3lDn1qfRFUl33auh8Eo4N8Z3Er4LrYbKGeDpKpS+S6gbsyNjTNc9QuAjGvRy6\nvbS0m0pfJNVl2j13WnJiEFROhTGrfScJNJW+SKrLwHvutKhoAUzUqZsdodIXSXUZeRC3BcXXwITn\n6djD3jObSl8k1WlP/68OToL6bpDzvu8kgaXSF0l1mfbwlFYZFF0DE5/3HSSwVPoiqaw70OkUHD/H\nd5LUoXH9DlHpi6Sygeh0zabKZ8HgrdDzoO8kgaTSF0llA9DQTlP13WDnHBj/ou8kgaTSF0llA9FB\n3OZoXL/dVPoiqWwQcOA83ylST/HnQlfnZtX6ThI4MZW+mc03s+1mVmRmdzczf6aZbTCzWjO7rsm8\nm8PrFZrZ1+MVXCQjDAYOTPadIvUcGwpHxsHIt3wnCZyopW9mWcCDwDzgfOAGMzu3yWLlwM3Ak03W\nzQZ+BEwHLgXuM7N+ccgtkvbqGurCY/qTfEdJTRriaZdY9vRnAMXOuXLnXC2wHFgUuYBzbpdz7kPO\nvkxuHrDKOfeRc64aWAXMj0NukbRXeqQUjgK1PX1HSU075oeGeKRNYin94cDuiM97wtNi0XTdijas\nK5LRCg4UwAHfKVLY3unQbzf02es7SaCkzJOzli5devp9Xl4eeXl53rKIpIKt+7eq9FvT0BlK58C4\nVbD5Ft9pkiI/P5/8/PwObSOW0q8ARkV8HhGeFosKIK/Jun9pbsHI0hcRKDioPf2oSuaFhngypPSb\n7hAvW7aszduIZXhnHTDezHLNrCuwGFjRyvKRlw6+DMw1s37hg7pzw9NEJAoN78RgxzwY9wpYve8k\ngRG19J1z9cCdhA7CbgWWO+e2mdkyM1sAYGaXmNlu4HrgV2b2QXjdI8D9wHrgXWBZ+ICuiLSivqGe\nwoOFoDsNtO7jkXAsB4Zu9J0kMMw5//elNjOXCjlEUkXJ4RKuevwqdn1/Fx27d7x5XD9J3z3v7+CT\nbHj9h2esmwmdYmY459p0YyZdkSuSggoOFDB5sC7KismOeTBeo8axUumLpKCCAwVMHqTSj0n5LMjZ\nAt0+8p0kEFT6Iimo4KD29GNW1wN2XwZjX/OdJBBU+iIpSMM7bdR46qZEpdIXSTENroFtB7Zx3mDd\nXTNmO+bD+JfQA9OjU+mLpJjdH+2mX/d+9O/e33eU4DhwHlgDDCr0nSTlqfRFUoyGdtrDNMQTI5W+\nSIrRmTvtpFM3Y6LSF0kx2tNvp9LPwKg3oPNJ30lSmkpfJMXodM12OpkN+y8IFb+0SKUvkkIaXIP2\n9Dtix7zwWTzSEpW+SArZcXgH2d2zGdhzoO8owaSDuVGp9EVSyIa9G5g2bJrvGMFVMT30JK0+voOk\nLpW+SArZWLmRaUNV+u3mOoUO6I7zHSR1qfRFUsiGyg1MHTrVd4xgK5kH432HSF0qfZEU4ZzTnn48\nlFwNY0MPopGzqfRFUkTpkVL6duvL4F6DfUcJto9HwDFYv3e97yQpSaUvkiI0tBNHxfBC8Qu+U6Qk\nlb5Iitiwd4OGduKlCFYWr/SdIiWp9EVSxMZ9G3W6Zrzshp1HdrL36F7fSVKOSl8kBTjn2LBXwztx\n0wDzxs/TEE8zVPoiKaCsuoweXXowpPcQ31HSxjUTruH54ud9x0g5Kn2RFKBTNeNv/vj5rN65mpq6\nGt9RUopKXyQF6Myd+BvUcxAXnHMBa8rX+I6SUlT6IilgQ6XO3EmEBRMWsLJIZ/FEUumLeNZ4EFdn\n7sTfNROvYWXRSpzTA9MbqfRFPNv98W46Z3VmaO+hvqOknQvPuZC6hjq2H9zuO0rKUOmLJNiQIaMx\nsxZfuZflUrWpiqysrLPmSceYmc7iaSKm0jez+Wa23cyKzOzuZuZ3NbPlZlZsZm+b2ajw9FwzO2Fm\nG8Ovh+L9GxBJdVVV5YBr+TV6CZQ90MJ86ahrJ13Ls9uf9R0jZUQtfTPLAh4E5gHnAzeY2blNFvsm\ncNg5NwH4OfBAxLwdzrmp4deSOOUWSR+j86HsSt8p0tacsXMoOFBAxccVvqOkhFj29GcAxc65cudc\nLbAcWNRkmUXAY+H3TwNzIubp36giLelVFXrSU+XFvpOkra6durJw0kKe2faM7ygpIZbSHw7sjvi8\nJzyt2WWcc/VAtZkNCM8bbWYbzOwvZnZFRwOLpJXR+bBrZuiJT5IwX5r8JZ4qeMp3jJSQqAO5jXv3\nlcAo59w04AfA78ysd4K+UyR4xvwFyvJ8p0h7c8fO5YP9H1B5tNJ3FO86x7BMBTAq4vOI8LRIe4CR\nwF4z6wT0dc4dDs87BeCc22hmJcBEYGPTL1m6dOnp93l5eeTl5cX2OxAJstH5sP523ynSXrfO3Vgw\ncQHPbHuGb8/4tu847Zafn09+fn6HtmHRLloIl3ghoXH6SuA94Abn3LaIZZYAFzjnlpjZYuDzzrnF\nZjaI0AHeBjMbC6wBLnTOVTf5DqeLJyRdhU69bObnu89eWHIBPHAQXEv/6G5h3di/3eP6fr+7aaes\nKFzBz97+Gfm35HcgU2oxM5xzbTpuGnV4JzxGfyewCtgKLHfObTOzZWa2ILzYw8AgMysG7gLuCU+f\nBbxvZhuBPwK3Ny18kYw1Oh/KZrdS+BJPV4+7mi1VW9h3bJ/vKF5F3dNPSgjt6Usaa3FPf+FtsP8C\nePd7ra3d/Lqxf7vH9VNrTx/gxmdu5IqRV3DH9Ds6kCt1JGRPX0QSROfnJ53O4lHpi/jRdw90rw7t\n6UvSzBs3j037NmX0hVoqfREfRv8FyjWen2w9uvTgy5O/zKObH/UdxRv9xIn4MGa1zs/35LZpt/Hw\npodpcA2+o3ih0hdJtqxamPQcFC70nSQjTRs6jb7d+rJ652rfUbxQ6Ysk29hX4dAE+CjXd5KMZGbc\nOvVWfrPxN76jeKHSF0m2C/4AW7/iO0VGu/HCG3lpx0scPHHQd5SkU+mLJFOnGpi0ArZ+yXeSjJbd\nI5uFkxbyxJYnfEdJOpW+SDKNfzl0mubRpjeqlWS7bept/HrjrzPu+bmx3HBNROLl/D/Ah4t9p8gA\n3WJ73OR3ICs368ybxwM5Obns21eWkGS+aU9fJFk6fwITn4eCL/pOkgFqaPURlY2v934Bl33+rOmh\nR1ymJ5W+SLJMeAH2XgLHc3wnkUYbb4UR70DO+76TJI1KXyRZLvgDfKizdlJKbU94+wcw6198J0ka\nlb5IMnSvhnEvw7brfCeRptZ/C3LXwOAC30mSQqUvkgzT/wsKr4VPBvpOIk2d6g3v3AUzf+w7SVKo\n9EUSrQvw6V/AG/f6TiItWfdtGLcKBhb5TpJwKn2RRJsGlM+EA5N9J5GW1PSF976TEWP7Kn2RBKqp\nq4HLgLX/5DuKRPPOXaFbXueu8Z0koVT6Ign06OZHoQqonOY7ikRT0xdeeBCuvS2tL1tV6YskSF1D\nHT9986ew1ncSiVnhItg3BWb7DpI4Kn2RBHl086OM7DcSdvlOIm3ywn/CVNhYudF3koRQ6YskwM4j\nO7n3tXv5xfxf+I4ibXU8B16BW1fcSm19re80cafSF4mzuoY6bnzmRu65/B6mDJniO460x2YY0XcE\n31r5rbS7C6dKXyTOfvz6j+ndtTff/5vv+44iHfD7L/6eD/Z/wH359/mOEldpfIxaJPne2v0W/73+\nv9l4+0ayTPtUQdaray9WfnUll//2cob3Gc7tl9zuO1JcqPRF4qTgQAFfefor/M+C/2FYn2G+40gc\nnNPrHF668SVmPjKT/t3785ULgn/DPO2KiMTBm7ve5MrHruRf5/wri85d5DuOxNG4AeN44cYX+MdX\n/5G7X7mbuoY635E6xFLhIIWZuVTIIdIeKwpXcOuKW3niC08wb/y8s+aHnuDU3p/vjqzre/1gf3fT\nTjp44iA3PnMjNXU1LL9+OUN6D+nA9uPDzHDOxfCIsIh1YilbM5sP/JzQvwweds79tMn8rsDjhO4y\nchD4inNuV3jevcDfAnXA95xzq5rZvkpfAufgiYPcv+Z+/ljwR1YsXsH04dObXU6lH8Tv7k7o6VvN\nbHY2oaZ7A9hAqNmaSNbjFhNS+maWBRQBc4C9wDpgsXNue8QydwAXOueWmNlXgC845xab2WTgSWA6\nMAJ4FZjQtOFV+vGVn59PXl6e7xhp4U9/+hO33PItunTpfnqa6+SomXKMk1OP0rWoB93f60vWJ52a\nXb9///7s3PkhwSzOjq7f0rr5QJ6n747T+kM2Qd5SGLYB1t4Lm2+B2l5nrJ+MTmtP6cdyIHcGUOyc\nKw9/yXJgEbA9YplFQON5TU8D/xl+fy2w3DlXB5SZWXF4e++2JaS0jUo/frZseZ9jx8ZD58dh3Osw\n+QWY+BqUfRoevoeaQ+Oa2x887dSpK5KWNTjyia30U9i+i2H5/8Gw9TD7n+Ez98LOq2DbF6D4c3DC\nd8CWxVL6wznzWfF7CBV3s8s45+rN7CMzGxCe/nbEchXhaSIpp66hjgPHD1B5rJLKo5WUHillpT0H\nU4th3tTQ8223fRFe/SUcje3H2EwnyKW1vZfA71dAj8MwcSWc+2f47HehBuyrBvuAauDj8Os4cBJo\naHmTiR4aStRPZJv+uQGw8PcLE5EjIxV+UMiG32/wHSMhIv/J7CL++d30n9IOR4NrwLnQr/WunvqG\nehpcA7WOkfLXAAADrklEQVQNtZyqP8Wp+lOcrDvJsVPHOFpzlJr6Ggb1HMTQ3kMZ2mcouf1yGdFp\nBFtKd9Dr15dhp7oAL4O9DH1jy/vJJ/vi8duWVPfJANjy9dDLGqB/Jxj6VOiB67m7oO8e6Lsbeh6E\n7h9BfdfQXT1re0Bdd6jvFprW0Jmqhre58rErybKs0y/DMLPTv3aIc67VF/Bp4KWIz/cAdzdZ5kXg\n0vD7TsD+5pYFXmpcrsn6Ti+99NJLr7a/onV401cse/rrgPFmlgtUAouBG5os8xxwM6Gx+i8Bq8PT\nVwBPmtl/EBrWGQ+81/QL2nogQkRE2idq6YfH6O8EVvHXUza3mdkyYJ1zbiXwMPBE+EDtIUJ/MeCc\nKzCzPwIFQC2wRKfpiIj4kxIXZ4mISHJ4vQ2DmV1vZh+aWb2ZTW0y714zKzazbWZ2ta+MQWVm95nZ\nHjPbGH7N950paMxsvpltN7MiM7vbd56gM7MyM9tiZpvM7KxhXmmdmT1sZlVm9n7EtGwzW2VmhWb2\nspn1i7Yd3/fe+QD4AnDGk4jN7Dzgy8B5wGeBh6zDh6wz0s+cc1PDr5d8hwmS8EWJDwLzgPOBG8zs\nXL+pAq8ByHPOXeyca3rat0T3CKGfx0j3AK865yYROpZ6b7SNeC1951yhc66Ys0/xXET4oi7nXBnQ\neFGXtI3+omy/0xclOudqgcaLEqX9DP87moHlnHsDONJk8iLgsfD7x4DPR9tOqv4HaHpBmC7qap9v\nm9lmM/tNLP/skzM0d1GifgY7xgEvm9k6M7vNd5g0cY5zrgrAObcPOCfaCgm/XNDMXgFyIicR+o//\n/5xzzyX6+9NZa3+2wEPAPzvnnJn9C/Az4JvJTyly2uXOuUozGwy8YmbbwnuvEj9Rz8xJeOk75+a2\nY7UKYGTE5xHhaRKhDX+2vyZ0LYXErgIYFfFZP4Md5JyrDP96wMz+TGgITaXfMVVmluOcqzKzIcD+\naCuk0vBO5PjzCmCxmXU1szG0cFGXtCz8A9DoOuBDX1kC6vRFieFbhy8m9HMp7WBmPc2sd/h9L+Bq\n9DPZHsbZXXlL+P3NwP9F24DXu0GZ2ecJ3ZFzELDSzDY75z6ri7ri4gEzm0LojIkyID0e8JkkLV2U\n6DlWkOUAfzYzR6h3nmzu2RrSMjP7HaHbkw40s12E7mz8r8BTZva3QDmhsx5b3466VEQkc6TS8I6I\niCSYSl9EJIOo9EVEMohKX0Qkg6j0RUQyiEpfRCSDqPRFRDKISl9EJIP8fx91B6/Pr9g8AAAAAElF\nTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f4fefdcb550>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}