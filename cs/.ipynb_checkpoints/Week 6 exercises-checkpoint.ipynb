{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:c130b4b7c4df70c579a736ac7a8db9cc1261da34f5bcb904633f687b9ae2bac4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Week 6 exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. Leapfrog integration and HMC\n",
      "\n",
      "In this exercise we will experiment with the leapfrog integrator used in HMC. As a target, we will use the 2D correlated normal with $\\rho = 0.998$ also used on Lecture 11.\n",
      "\n",
      "1. Simulate the system for $L=10$ steps starting from the origin $x_0 = (0, 0)$ with momentum $r = (1, 1/2)$ with several values of $\\epsilon$ and plot the resulting acceptance rates at the end of the trajectory. What is the smallest $\\epsilon$ that will yield acceptance rate below 60%? What is the largest $\\epsilon$ that will yield an acceptance rate above 10%? (Required tolerance: 0.001.)\n",
      "2. Simulate the system for many steps with $\\epsilon=0.05$ starting from the origin $x_0 = (0, 0)$ with momentum $r = (1, 1/2)$. Plot the trajectory and the distance from the starting point at each point. How many steps do you need to take until the trajectory reaches the first local maximum distance from the origin? How many steps will lead you to the first local minimum?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "%matplotlib inline\n",
      "import autograd.numpy as np\n",
      "import autograd.numpy.random as npr\n",
      "import autograd\n",
      "import matplotlib.pyplot as plt\n",
      "import autograd.numpy.linalg as npl\n",
      "\n",
      "acceptance_rates = []\n",
      "\n",
      "def correlated_normal(x, rho=0.998):\n",
      "    Sigma = np.array([[1.0, rho],[rho, 1.0]])\n",
      "    return -0.5*(x.T @ npl.solve(Sigma, x))\n",
      "\n",
      "def hmc(x0, M, target, epsilon0, L):\n",
      "    xs = np.zeros([M, len(x0)])\n",
      "    gradF = autograd.grad(target)\n",
      "    x = np.copy(x0)\n",
      "    g = gradF(x)  # set gradient using initial x\n",
      "    logP = target(x)  # set objective function too\n",
      "    accepts = 0\n",
      "    for m in range(M): # draw M samples after M warm-up iterations\n",
      "        #p = npr.normal(size=x.shape)  # initial momentum is Normal(0,1)\n",
      "        p = np.array([1,.5])\n",
      "        H = p.T @ p / 2 - logP   # evaluate H(x,p)\n",
      "        xnew = np.copy(x)\n",
      "        gnew = np.copy(g)\n",
      "        for l in range(L): # make L \u2018leapfrog\u2019 steps\n",
      "            #epsilon = npr.uniform(0.8, 1.2) * epsilon0  # optional: randomise epsilon for improved theoretical convergence properties\n",
      "            epsilon = epsilon0\n",
      "            p = p + epsilon * gnew / 2   # make half-step in p\n",
      "            xnew = xnew + epsilon * p    # make step in x\n",
      "            gnew = gradF(xnew)           # find new gradient\n",
      "            p = p + epsilon * gnew / 2   # make half-step in p\n",
      "        logPnew = target(xnew)   # find new value of H\n",
      "        Hnew = p.T @ p / 2 - logPnew\n",
      "        dH = Hnew - H    # Decide whether to accept\n",
      "        #print(np.exp(-dH))\n",
      "        #acceptance_rates\n",
      "        if dH < 0 or np.log(npr.rand()) < -dH:\n",
      "            g = gnew\n",
      "            x = xnew\n",
      "            logP = logPnew\n",
      "            accepts += 1\n",
      "        if m >= M:\n",
      "            xs[m-M,:] = x\n",
      "    #print('Acceptance rate:', accepts/(2*M))\n",
      "    return np.exp(-dH)\n",
      "\n",
      "\n",
      "\n",
      "def hmc_trajectory(x0, M, target, epsilon0, L):\n",
      "    xs = np.zeros([M, len(x0)])\n",
      "    gradF = autograd.grad(target)\n",
      "    x = np.copy(x0)\n",
      "    g = gradF(x)  # set gradient using initial x\n",
      "    logP = target(x)  # set objective function too\n",
      "    accepts = 0\n",
      "    for m in range(M): # draw M samples after M warm-up iterations\n",
      "        #p = npr.normal(size=x.shape)  # initial momentum is Normal(0,1)\n",
      "        p = np.array([1,.5])\n",
      "        H = p.T @ p / 2 - logP   # evaluate H(x,p)\n",
      "        xnew = np.copy(x)\n",
      "        gnew = np.copy(g)\n",
      "        for l in range(L): # make L \u2018leapfrog\u2019 steps\n",
      "            #epsilon = npr.uniform(0.8, 1.2) * epsilon0  # optional: randomise epsilon for improved theoretical convergence properties\n",
      "            epsilon = epsilon0\n",
      "            p = p + epsilon * gnew / 2   # make half-step in p\n",
      "            xnew = xnew + epsilon * p    # make step in x\n",
      "            gnew = gradF(xnew)           # find new gradient\n",
      "            p = p + epsilon * gnew / 2   # make half-step in p\n",
      "        logPnew = target(xnew)   # find new value of H\n",
      "        Hnew = p.T @ p / 2 - logPnew\n",
      "        dH = Hnew - H    # Decide whether to accept\n",
      "        #print(np.exp(-dH))\n",
      "        #acceptance_rates\n",
      "        if dH < 0 or np.log(npr.rand()) < -dH:\n",
      "            g = gnew\n",
      "            x = xnew\n",
      "            logP = logPnew\n",
      "            accepts += 1\n",
      "        if m >= M:\n",
      "            xs[m-M,:] = x\n",
      "    #print('Acceptance rate:', accepts/(2*M))\n",
      "    return x\n",
      "\n",
      "a = []\n",
      "b = []\n",
      "epsilon = np.linspace(.084, .088, num=100)\n",
      "for i in epsilon:\n",
      "    rate = hmc(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), i, 10)\n",
      "    #print(rate)\n",
      "    a.append(i)\n",
      "    b.append(rate)\n",
      "    \n",
      "rate = hmc(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), .08635, 10)\n",
      "print(rate)\n",
      "    \n",
      "#plt.plot(a,b)\n",
      "#plt.plot(samples1[:,0], samples1[:,1], '.')\n",
      "#print(np.mean(samples1, 0), np.std(samples1, 0))\n",
      "\n",
      "\n",
      "a = []\n",
      "b = []\n",
      "epsilon = np.linspace(0.088, .089, num=100)\n",
      "for i in epsilon:\n",
      "    rate = hmc(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), i, 10)\n",
      "    #print(rate)\n",
      "    a.append(i)\n",
      "    b.append(rate)\n",
      "    \n",
      "rate = hmc(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), .0888, 10)\n",
      "print(rate)\n",
      "    \n",
      "#plt.plot(a,b)\n",
      "\n",
      "\n",
      "#2nd part\n",
      "a = []\n",
      "b = []\n",
      "#epsilon = np.linspace(1, .089, num=100)\n",
      "for i in range(85,92):\n",
      "    x = hmc_trajectory(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), .05, i)\n",
      "    #print(x)\n",
      "    a.append(i)\n",
      "    b.append(np.sqrt(x[0]**2 + x[1]**2))\n",
      "    \n",
      "#rate = hmc(np.array([0, 0.0]), 1, lambda x: correlated_normal(x), .0888, 10)\n",
      "#print(rate)\n",
      "    \n",
      "plt.plot(a,b)\n",
      "\n",
      "#plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.541369366753\n",
        "0.139880740686"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 139,
       "text": [
        "[<matplotlib.lines.Line2D at 0x7f099c7f76a0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVOX1x/HPEcRYImBXsCtKsBewOwQJYMMeMCIxxBYV\n5WcBKxglLE1BEURUYgNsKFhBhJVgAQQRpBcjCEoEKWrEAD6/P84gy7rsLrt35k75vl+vfWVn9u6d\nw+R65uxzn+c8FkJARERy11ZxByAiIqmlRC8ikuOU6EVEcpwSvYhIjlOiFxHJcUr0IiI5rsxEb2ZP\nmNlSM5tayjEJM/vEzD4zszHRhigiIpVhZc2jN7NTgO+Bp0MIR5Tw8+rAB8AfQgiLzWyXEMKylEQr\nIiJbrMyKPoQwDlhRyiGXAi+HEBYnj1eSFxHJIFGM0dcBdjKzMWY20cxaRXBOERGJSNWIznEM8Htg\ne+BDM/swhDAvgnOLiEglRZHovwSWhRDWAGvMbCxwJPCrRG9maqwjIlIBIQSr6O+Wd+jGkl8lGQac\nYmZVzGw7oAEwc3MnCiHoKwQ6duwYewyZ8qX3Qu+F3ovSvyqrzIrezAYBCWBnM1sIdASqec4Oj4UQ\nZpnZCGAqsB54LIQwo9KRiYhIJMpM9CGES8txTA+gRyQRiYhIpLQyNiaJRCLuEDKG3ouN9F5spPci\nOmUumIr0xcxCOl9PRCQXmBkhDTdjRUQkSynRi4jkOCV6EZEcp0QvIpLjlOhFRHKcEr2ISI5TohcR\nyXFRNDUTSYlvv4Xp02HOHP/64gtYvtyfX7nSj6lSxb923BF23dW/atWCunWhXj045BDYdtt4/x0i\ncdOCKckYixbBW2/BBx/Ahx/CV195sq5TxxP2vvvCLrvAzjtDjRr+O+vX+9eqVfDNN7BsGSxcCDNn\nwowZMH++n+O00+D00yGR8A8FkWxS2QVTSvQSq9mz4fnnYdgw+Pe/oVkzOPVUOPFET9BVqlTu/GvW\nwMSJMHYsFBbC+PFw8slw3nn+tfvuUfwrRFJLiV6yzurV8MIL8OST8Pnn8Mc/etI95RSomuLBxNWr\n4e234ZVX/H8bNoSrr4bGjWEr3bGSDKVEL1lj0SLo1Qv++U8fRrniCmjaFLbeOp54Vq+GwYOhf38f\n82/XDtq0ge22iycekc1RrxvJePPmweWXw5FH+uMpU2DoUDjnnPiSPPhY/dVXw6RJMGgQjBkD++8P\n99+/8WavSC5QopeU+fpruO46OOEEOPhgWLAAevaEvfeOO7JNmXmMQ4f6OP68eX4D+MEH4aef4o5O\npPKU6CVyP/3kVXG9erDNNjBrFtx998aZMpmsbl0fWhozBkaP9sfPPw8acZRspjF6idTIkXD99fC7\n33lFvP/+cUdUOYWFcOONsOee0LcvHHBA3BFJPtIYvWSEZcugRQu45hpP8K++mv1JHnze/ccfw+9/\nD/XrQ9eusG5d3FGJbJkyE72ZPWFmS81sahnHHW9ma83sgujCk2zw+utwxBFQu7avZD3rrLgjitbW\nW8Ntt8GECTBqlE8DnTMn7qhEyq88Ff1AoElpB5jZVkABMCKKoCQ7fP89XHkltG0LQ4ZAjx653W7g\ngANgxAi47DI46STo109j95Idykz0IYRxwIoyDrsBeAn4TxRBSeabMcOHMtauhU8/9RYD+WCrrfwe\nxLhxvuCreXNYUdZ/HSIxq/QYvZntBZwXQugHVPhmgWSPZ5/1BU+33uozVH7727gjSr9DD4X334f9\n9oPjjvO1ASKZKooF572A9kUeK9nnqLVr4aabfJz63Xd9XD6fVasGDz3kfXkaN/ahq9at445K5Nei\nSPTHAUPMzIBdgGZmtjaEMLykgzt16vTL94lEgkQiEUEIkmrLl8PFF/sY/IQJUL163BFljpYt/UPv\nvPP8ZnSXLpVvxib5rbCwkMLCwsjOV6559Ga2H/BaCOHwMo4bmDxu6GZ+rnn0WWjmTG9XcP75UFCg\nJLY5y5fDBRdAzZrw3HOw/fZxRyS5IuXz6M1sEPABUMfMFprZFWZ2tZldVcLhyuI5ZuxYn0t+993Q\nvbuSfGl23hneeccT/amnwpIlcUck4rQyVjbr1Vfhqqu84dcZZ8QdTfYIATp39lk5I0Z4nx+Ryqhs\nRa+tBKVEAwZAx46+49Oxx8YdTXYxg7vu8k1NTj/dF5Qdc0zcUUk+U6KXX+nRw/u6vPeeqtHKuPJK\n3/qwaVPfaEXzDiQuGrqRTdx/Pzz9tHdurF077mhyw5gxvovW4MHQqFHc0Ug2UlMziUQIfsN10CCv\n5JXko9OwIbz0kk/DHDUq7mgkHynRCyHAnXf6Bt2Fhd6SV6J12mm+scmll3orZ5F0UqIXOneG4cN9\nuGa33eKOJnedcopvSv6nP/lwjki6KNHnuQce8DH5UaP8xqGk1skn+43ZSy6Bjz6KOxrJF0r0eezR\nR+Hhh71vzR57xB1N/mjY0JvBNW+uZmiSHkr0eerFF32GzahRmbdZdz446yzo0weaNdMmJpJ6mkef\nhwoL4brr/KbggQfGHU3+uvhiWLXKk/0HH/gCK5FUUKLPM9Om+ZzuIUPgqKPijkb++ldYvBjOPNM/\ngPOxt7+knhZM5ZFFi3wLvO7dfSNvyQwheE+hRYvgtdd8j1qRoiq7YEqJPk98951P72vVCm65Je5o\npLh167yf/e67w+OPe78ckQ2U6KVM69b5DI/atX2mjZJIZvr++40fxjffHHc0kknUvVLKdPPN8L//\n+SwPJfnMtcMOvnDtxBPhkEPg7LPjjkhyhaZX5ri+fX0K5Ysvauw3G+yzD7z8Mlxxhd84F4mChm5y\n2Hvv+QybDz6AAw6IOxrZEoMGef+hiRO1Ylk0Ri+bsXAhNGjg7Q0aN447GqmIDh080Y8YAVU1yJrX\nlOjlV3780fcs/eMf4dZb445GKmr9ep9ff9hh0LNn3NFInNKxOfgTZrbUzKZu5ueXmtmnya9xZnZ4\nRYORygsBrr3Wd4bSNMrsVqWKb1by6qs+lCNSUeW5GTsQaFLKzxcAp4UQjgTuBwZEEZhUzIABMGmS\n5mLnip128kR/441qgCYVV66hGzPbF3gthHBEGcfVAKaFEEpsk6Whm9SaNMn7powbB3XqxB2NRGnw\nYLjnHv//eMcd445G0i3TthL8K/BWxOeUclixwptkPfKIknwuatnS95v96199eE5kS0R2L9/MGgJX\nAKeUdlynTp1++T6RSJBIJKIKIW+FAH/+M5xzjid7yU29evliqr59vfuo5K7CwkIKCwsjO18kQzdm\ndgTwMtA0hDC/lPNo6CYFHngAnn8e/vUvqFYt7mgklebN82T/5ptw/PFxRyPpkq6hG0t+lRTAPniS\nb1VakpfU+PhjKCjwtsNK8rnvoIO8om/RAlavjjsayRZlVvRmNghIADsDS4GOQDUghBAeM7MBwAXA\nF/iHwdoQQv3NnEsVfYRWr4ZjjoEuXTRkk2+uvBLWrIFnnok7EkkHLZjKUyHAZZfB9tvDY4/FHY2k\n2w8/wHHHeZuEyy6LOxpJNXWvzFNPPeXzqidOjDsSicP22/uUy8aNfcxeW0JKaVTRZ6EFC6B+fRgz\nBg7XOuS81ru3r5odN07dSXNZps2jlxRbvx4uvxxuv11JXqBtW6hZEzp3jjsSyWSq6LNMQYF3M3z3\nXdhKH9MCLFkCRx/tm5Y0aBB3NJIKuhmbRz75BP7wB18Gv88+cUcjmeSll/yvvClTfPxecosSfZ5Y\ns8ZnWXTooFkWUrLWrWG77aBfv7gjkagp0eeJ9u19VeRLL6krpZRs1So48khfUHXmmXFHI1FSos8D\nH34I558PU6fCbrvFHY1kstGjvbL/7DOoXj3uaCQqSvQ57r//9RttnTvDRRfFHY1kg2uvhbVrfU8C\nyQ1K9DmuXTv4+mtfHCNSHt9951Nv+/eHJqVtGSRZQ4k+h40bB5dcAtOmwc47xx2NZJNRo+Avf/Fr\nR0M42U+JPketWeM31goKfHxeZEtdfbX3RFIvpOynRJ+j7rgD5szxWTYiFbF6NdSr5x0utb9PdlOi\nz0GffOJjq1Onwh57xB2NZLPhw+Hmm/1a2nbbuKORilKvmxyzbh20aQNduyrJS+Wde67vWVBkB0/J\nQ6roM0y3bvDOOzBypBZGSTSWLoUjjoC33vKkL9lHQzc55PPPfR/QCRPggAPijkZyyVNPeUvjCROg\nqnahyDoauskRIcDf/ga33KIkL9G7/HKfovvQQ3FHInFQRZ8hXngB7rsPJk/WBhKSGnPn+m5Ukyer\n+2m2SXlFb2ZPmNlSM5tayjEPmdlcM5tiZkdVNJh8tWqVr4Dt319JXlLn4IN9o5K2beOORNKtPEM3\nA4HNLqQ2s2bAgSGEg4GrgUcjii1v3HEHnH02nHRS3JFIrmvfHmbNgmHD4o5E0qnM2zIhhHFmtm8p\nhzQHnk4eO97MqpvZ7iGEpVEFmcsmToShQ2HGjLgjkXywzTber751a2jUCHbYIe6IJB2iuBlbC1hU\n5PHi5HNShvXrvdNg166+76dIOjRs6F/33ht3JJIuaZ9o1anIyo1EIkEij9dmP/aY7wjUqlXckUi+\n6dYNDjsMrrgCfve7uKOR4goLCyksLIzsfOWadZMcunkthHBECT97FBgTQng++XgWcHpJQzeadbPR\nf/7jfUjGjPH/4ETS7eGH4dVXvdOlFudltnTNo7fkV0mGA5cngzkBWKnx+bLddpuPkyrJS1yuvRaW\nLfOpvZLbyqzozWwQkAB2BpYCHYFqQAghPJY8pg/QFPgBuCKEMHkz51JFj/eZb9nSb8D+9rdxRyP5\nbMO1OHOmbsxmMrVAyDLr18Nxx/k0txYt4o5GxFfN7rGHj9tLZlKizzL9+8Nzz8F772lcVDLD11/7\nEOL778Mhh8QdjZREiT6LfPst1K0LI0bAUVo/LBmkZ0+/KfvmmypAMpESfRa54QYfuunbN+5IRDb1\nv/95K+MePXyVtmQWJfosMW2ar0ScOVMbfUtmevttuP56mD7dV9BK5lCb4iwQAtx0E9xzj5K8ZK6m\nTX1tx4MPxh2JRE0VfRoMG+aNyz79VJs+SGabPx8aNPA9ZvfaK+5oZAMN3WS4n37yGQ2PPAJ/+EPc\n0YiUrX17X7k9cGDckcgGSvQZrmdPKCyE116LOxKR8lm9GurU8Rk42mM2MyjRZ7BvvvGGUePGaX6y\nZJf+/WHwYO/FpOmW8dPN2Ax2zz1w2WVK8pJ92rSB5cu96ZlkP1X0KfLZZz6dctYs9ZqX7PTOO974\nTNMt46eKPkPdeivceaeSvGSvxo3h0EN9IoFkN1X0KTBypC88+ewzqFYt7mhEKm7mTDj9dP/LdKed\n4o4mf6mizzDr18Mtt/j2gEryku3q1oULLoDOneOORCpDFX3EHn8cnn5a3Skld3z9ta+YnTgRDjgg\n7mjyk6ZXZpDvv/f5x8OGwfHHxx2NSHTuu89vyg4ZEnck+UmJPoPcey/Mng2DBsUdiUi0fvjBpwm/\n/LK3SJD0UqLPEEuX+uKojz+G/fePOxqR6D35pLdFGDtWw5LpppuxGeL++6FVKyV5yV2tW8PKlWrn\nkY3KlejNrKmZzTKzOWbWvoSf721mo81ssplNMbNm0YeauebP9+Xid94ZdyQiqVOlChQUQIcOsG5d\n3NHIligz0ZvZVkAfoAlQD2hpZocWO+wu4PkQwjFASyCv9lC66y5o1w523TXuSERS68wz/Tp/6qm4\nI5EtUZ6Kvj4wN4TwRQhhLTAEaF7smJ+BHZPf1wAWRxdiZvv4Yx+zvOmmuCMRST0z6NYNOnaE//43\n7mikvMqT6GsBi4o8/jL5XFH3Aq3MbBHwOnBDNOFlvg4d4O67Yfvt445EJD0aNIATT4TeveOORMor\nqv2OWgIDQwgPmtkJwLP4MM+vdOrU6ZfvE4kEiUQiohDSb9QoWLjQO/2J5JPOneGkk+Cqq7Q9ZioU\nFhZSWFgY2fnKnF6ZTNydQghNk487ACGE0LXIMZ8BTUIIi5OP5wMNQgjLip0rZ6ZXhuCLom67DS65\nJO5oRNLvmmtgxx19KEdSKx3TKycCB5nZvmZWDWgBDC92zBfAGcmA6gLbFE/yueall/x/L7oo3jhE\n4nL33d7yY3He3JHLXuVaMGVmTYHe+AfDEyGEAjO7F5gYQng9mdwHADvgN2ZvDSG8W8J5cqKiX7vW\ne3888oi3chXJV7fe6q0/+vWLO5LcppWxMRgwwHt+jBqlFYKS35Yv99YI48fDgQfGHU3uUqJPsx9/\nhIMPhqFDoX79uKMRid/f/w5z5sCzz8YdSe5Sok+znj3h/fc90YsIfPcdHHSQ/4V7+OFxR5OblOjT\naMMFPXq0j9GLiHvwQd+DQZuJp4aamqVRr15+81VJXmRT114LkybBhAlxRyIlUUVfTt9+65uKfPSR\nV/Uisqn+/b1f/ciRcUeSe1TRp0n37r53ppK8SMn+8hfv5Pree3FHIsWpoi+HDZuKTJkCe+8ddzQi\nmeuZZ7yy/9e/NPU4Sqro06CgAC67TElepCyXXgorVsBbb8UdiRSlir4MS5bAYYfBjBmwxx5xRyOS\n+V5+2ZueTZqkqj4qquhTrEsXH3tUkhcpn/PP96Z/w4bFHYlsoIq+FIsWwVFHwcyZsNtucUcjkj2G\nD/emZ598AlupnKw0VfQp9I9/wJVXKsmLbKlzzoGtt4ZXXok7EgFV9Jv173/DscfC7Nmwyy5xRyOS\nfd580/drmDpVVX1lqaJPkfvv99V+SvIiFdOsGfz2t/Dii3FHIqroS7Bgge8eNXcu7LRT3NGIZK93\n3oG2beGzz6BKlbijyV6q6FPg/vvhuuuU5EUq64wzfE/Z55+PO5L8poq+mHnz4IQTvJqvWTPuaESy\nn6r6ylNFH7H774frr1eSF4nKGWf4f08vvBB3JPlLFX0RG6r5efOgRo24oxHJHSNHwk03wbRpquor\nIi0VvZk1NbNZZjbHzNpv5phLzGy6mU0zs6zcVOy++/xPTCV5kWg1bgzVq2sGTlzKrOjNbCtgDtAI\nWAJMBFqEEGYVOeYg4HmgYQhhtZntEkJYVsK5MrainzsXTjrJq/nq1eOORiT3vP023HyzV/WaV79l\n0lHR1wfmhhC+CCGsBYYAzYsdcyXwSAhhNUBJST7T/eMfcMMNSvIiqdKkCeywg6r6OJQn0dcCFhV5\n/GXyuaLqAIeY2Tgz+8DMmkQVYDosWOC9Odq2jTsSkdxlBh07+hDpzz/HHU1+qRrheQ4CTgP2Acaa\n2WEbKvyiOnXq9Mv3iUSCRCIRUQgVV1AAf/ubxuZFUq1ZM0/2r7wCF14YdzSZq7CwkMLCwsjOV54x\n+hOATiGEpsnHHYAQQuha5Jh+wEchhKeSj0cB7UMIk4qdK+PG6BcuhKOPhjlzfGGHiKTWa6/BXXep\ns+WWSMcY/UTgIDPb18yqAS2A4cWOeRVomAxoF+BgYEFFg0qnrl29Q6WSvEh6nH02VK3qw6WSHmUm\n+hDCeuB6YCQwHRgSQphpZvea2dnJY0YAy81sOvAucEsIYUUK447E4sUweDD83//FHYlI/jCDe+6B\nv//dNyiR1MvrBVM33eSLN3r2jDsSkfwSgg+Z3nef966X0lV26CZvE/3SpVC3LkyfDnvuGXc0Ivln\n6FDfqnPCBO0tWxb1uqmgBx7wHeuV5EXicd558OOP3h5BUisvK/rly6FOHb/rv88+cUcjkr8GDYJ+\n/eBf/4o7ksymir4CeveGCy5QkheJ2yWXwNdfw9ixcUeS2/Iu0a9aBX37wu23xx2JiFStCh06eHtw\n2eiBB+Crr6I7X94l+j594Mwz4YAD4o5ERABatYJZs/ymrMCMGdCtW7R9t/JqjP777z3Bjx0Lhx4a\nWxgiUkyfPr4T1bBhcUcSv1at4He/23TUQdMrt0DPnjB+vHa6Eck0P/7oRdiIEXDEEXFHE58FC6B+\nfZg/f9OKXom+nNas8QvpzTfhqKNiCUFEStGtm8+EGzw47kjic8013o6lc+dNn1eiL6d+/eCNN+D1\n12N5eREpw3ffeTH2wQdw8MFxR5N+S5bAYYfB7Nmw666b/kyJvhzWrvULZ/BgOPHEtL+8iJRTx47e\ng+rxx+OOJP1uuQXWrYNevX79MyX6cvjnP+Hpp2H06LS/tIhsgeXLvSj79FPYe++4o0mfDf/uqVOh\ndu1f/1wLpsqwfr3307jzzrgjEZGy7LwztGkDPXrEHUl6PfSQL+IsKclHIecr+hdf9Nk2H36oxkki\n2eCrr6BePZ9bv9tucUeTeuW5N6GKvhQh+DaBt9+uJC+SLfbcE1q0KHmsOhf16wdnnJHaG9A5XdGP\nHAnt2sG0adqyTCSbfP45HH/8r+eT55ryrh9QRV+KggJo315JXiTb7L+/byTet2/ckaTWwIH+gZbq\nRWI5W9GPH++d8ebNg623TstLikiEpk+HRo18teh228UdTfQ2TPseMgROOKH0Y1XRb0ZBgc9LVZIX\nyU716nkCfPLJuCNJjUGD4MADy07yUShXojezpmY2y8zmmFn7Uo670Mx+NrNjogtxy82c6Xew27SJ\nMwoRqazbb4fu3b36zSU//7xxokg6lJnozWwroA/QBKgHtDSzX/V+NLMdgLbAR1EHuaW6doUbbsjN\nP/dE8kmDBnDQQV795pLXXoMddvChqXQoT0VfH5gbQvgihLAWGAI0L+G4+4AC4KcI49tiCxf6m3jd\ndXFGISJRueMOr35//jnuSKIRgi/i7NAhfdO+y5PoawGLijz+MvncL8zsaKB2COGtCGOrkB49fMim\nZs24IxGRKPz+9179Dh8edyTRGDsWVqzwzdHTpWplT2BmBjwAtC769OaO79Sp0y/fJxIJEolEZUP4\nxTffwLPP+t16EckNZj5NumtXaN48+xc/FhTArbdClSqbP6awsJDCwsLIXrPM6ZVmdgLQKYTQNPm4\nAxBCCF2Tj3cE5gHf4wl+D2A5cG4IYXKxc6V0euVdd8GyZfDooyl7CRGJwfr1vivcE0/AaafFHU3F\nTZkCZ53lU0a32ab8v5fy7pVmVgWYDTQCvgImAC1DCDM3c/wY4P9CCJ+U8LOUJfrVq32F2fjxPmVJ\nRHJL//4+fPPGG3FHUnEtW8Kxx/rU7y2R8nn0IYT1wPXASGA6MCSEMNPM7jWzs0v6FUoZukmVRx+F\nxo2V5EVyVevWMHmytzTJRgsW+L64V12V/tfOiZWxG7YJfOstOPLIyE8vIhmiSxdfJ/P003FHsuWu\nu8779vzjH1v+u9p4BBgwAF55xfeDFZHctXKl/9U+eTLsu2/c0ZTff/7j9xhmzoTdd9/y38/7Fgjr\n1/vKudtuizsSEUm1GjV8+vQDD8QdyZZ5+GHvvVWRJB+FrK/ohw71aVcffZT9065EpGwbNtGeMwd2\n2SXuaMr2/ffejfPDD32Vb0XkdUUfgif59u2V5EXyxV57wYUXQp8+cUdSPo8/Dg0bVjzJRyGrK/rC\nQrj6apgxo/TFByKSW+bMgZNP9g1Kdtgh7mg2b+1av6cwdCgcd1zFz5PXFX23bmWvMBOR3FOnDiQS\nXi1nssGDved8ZZJ8FLK2op86FZo29U/0LVlhJiK54eOP4fzzfbvBatXijubXfv4ZDj/c975t3Lhy\n58rbir57d7jxRiV5kXx13HE+ZfG55+KOpGRvvOH56Ywz4o4kSyv6hQvh6KP9k7xGjQgCE5GsNGoU\ntG0Ln32WWXtDh+D3EG66yadVVlZeVvS9esEVVyjJi+S7Ro3gN7/JvP4348b5IqkLL4w7Epd1Ff2K\nFX4X+9NPYe+9IwpMRLLWkCHQt6/3ec8UZ58N55zjswKjkHcV/aOP+huoJC8iABddBIsW+YKkTDB1\nKkya5E3YMkVWVfRr1vgKs5Ej/W62iAh4i4ExY3y+etxatYJ69XyrwKjkVVMzNS8TkZL88APstx+8\n/77PsY/LokXeQXfBgmjvIebN0M369b4frJqXiUhx228Pf/sb9OwZbxwPPeRDNpk2USRrKvqXX/aV\nsGpeJiIl+eYbr+ZnzoQ99kj/669a5ftipKKFcl5U9GpeJiJl2XVXaNECHnkkntcfMACaNMnMPvlZ\nUdGPGQPXXKPmZSJSurlz4aST4N//9uGcdPnf/3za97BhcMwx0Z8/LRW9mTU1s1lmNsfM2pfw83Zm\nNt3MppjZO2YW6eTHrl3VvExEynbwwXDqqTBwYHpf94UXfNgoFUk+CmVW9Ga2FTAHaAQsASYCLUII\ns4occzowPoSwxsyuARIhhBYlnGuLK/opU+DMM9W8TETK58MP4U9/8lbGVaum/vVC8JYsXbpAs2ap\neY10VPT1gbkhhC9CCGuBIUDzogeEEN4LIaxJPvwIqFXRgIrr1s37RSjJi0h5nHgi7LmnT8VOh3fe\ngXXrvJtupipPoq8FLCry+EtKT+RtgLcqE9QGX3wBI0ZEt4xYRPLDLbd4h9t03ILcsC9GJk8UiXTW\njZldBhwLdI/ifL17e/Oy6tWjOJuI5Itzz4WVK1Pf/2byZJg1C1q2TO3rVFZ5RrAWA/sUeVw7+dwm\nzOwM4HbgtOQQT4k6der0y/eJRIJEIlHicatWwVNP+Ri9iMiWqFLFq/pu3eD001P3Ot27Q7t20W98\nUlhYSGFhYWTnK8/N2CrAbPxm7FfABKBlCGFmkWOOBl4EmoQQ5pdyrnLfjO3RAz75JHM3FRCRzLZm\njS9gevttOOKI6M//+edw/PHe7mDHHaM/f1Fp6XVjZk2B3vhQzxMhhAIzuxeYGEJ43czeAQ7DPwgM\n+CKEcF4J5ylXol+71v8PStWcVBHJDwUFvinJs89Gf+4bbvC5+gUF0Z+7uJxsavbcc77p75gxaQhK\nRHLWypW+kGnSJG96FpVly3zO/vTpsNde0Z13c3KuBUII3pjollvijkREsl2NGtCmDTzwQLTn7dsX\nLrggPUk+ChlX0Y8eDddd55+UmbQHpIhkpyVL4LDDfAHVLrtU/nz//a/vi/Hee745eTrkXEXfvbtX\n80ryIhKFvfbyvVv79InmfP/8py/KSleSj0JGVfRTp/rqMrU7EJEozZ4Np53mzc623bbi51m3Dg45\nBJ55xpuMq/hsAAAHFElEQVSnpUtOVfQ9esCNNyrJi0i0DjkE6tev/OyboUO9vUI6k3wUMqaiX7jQ\nGwPNn595u7OISPYbM8Z3oaro/b8QfN58x45wzjnRx1eanKnoe/XydgdK8iKSCokE/OY3voCqIsaM\n8RuxZ50VaVhpkREV/YoVPtd16lSoXTtt4YhInnn2We9V/+67W/67zZrBxRfDX/4SfVxlyYmKvl8/\nb0KkJC8iqXTJJX5jdkt7aM2Y4S1ZLr00NXGlWuwV/Y8/+pzUd9+FevXSFoqI5KmuXT1xP/VU+X/n\nqqu8EL3nntTFVZqsb4HQt6/3nB82LG1hiEge29Kh4g3tDmbPht12S318JcnqoZt163yBVIcOcUYh\nIvmkZk3485/hwQfLd/yjj/qCq7iSfBRireiffdabl0XYdllEpEyLFsGRR/p07po1N3/cTz95M7RR\no+IdWs7aiv7nn729p6p5EUm3vfeG5s196Lg0Q4Z4L/tsv38YW6J/4w3Yemto0iSuCEQkn912Gzz8\nsE8IKUkIPrzTrl1640qFWBJ9CNCli1fzmbyhrojkrrp1oUEDn1dfkuef9y0Jc6EYjSXRr1sHZ58N\nF10Ux6uLiLgOHbzH1rp1mz7/44/+swcfzI1iNPbplSIicUokoFEjuPvujc916QIffwwvvxxbWJuo\n7M3YqlEGIyKSbQYN8mRfrRq0bw9Ll/oudx99FHdk0SlXok9uDt6LjZuDdy3282rA08CxwDLgjyGE\nhRHHKiISub328oZlDRv6MM38+dC6NRx0UNyRRafMMXoz2wroAzQB6gEtzaz43iptgG9DCAfjHwjd\nog401xRq8cAv9F5spPdio3S+F7VqebIfMABeeQXuuittL50W5bkZWx+YG0L4IoSwFhgCNC92THNg\nQ+eIl4BG0YWYm/Qf9EZ6LzbSe7FRut+LWrVg7Fh4/fXSF1Flo/IM3dQCFhV5/CWe/Es8JoSw3sxW\nmtlOIYRvowlTRCT19tzTv3JNqqZX5sCEJBGR3FDm9EozOwHoFEJomnzcAQhFb8ia2VvJY8abWRXg\nqxDCr1oAmZnmVoqIVECqp1dOBA4ys32Br4AWQMtix7wGtAbGAxcDo6MOVEREKqbMRJ8cc78eGMnG\n6ZUzzexeYGII4XXgCeAZM5sLLMc/DEREJAOkdWWsiIikX9p63ZhZUzObZWZzzKx9ul43E5hZbTMb\nbWbTzWyambVNPl/TzEaa2WwzG2Fm1eOONR3MbCszm2xmw5OP9zOzj5LXxmAzy5sV22ZW3cxeNLOZ\nyeujQT5eF2bWzsw+M7OpZvacmVXLp+vCzJ4ws6VmNrXIc5u9DszsITOba2ZTzOyoss6flkRfzkVX\nuWwd8H8hhHrAicB1yX9/B2BUCOEQ/L7G7THGmE43AjOKPO4K9Awh1AFW4gvw8kVv4M0QQl3gSGAW\neXZdmNlewA3AMSGEI/Ah5Zbk13UxEM+PRZV4HZhZM+DA5ALVq4FHyzp5uir68iy6ylkhhK9DCFOS\n338PzARqs+lCs6eA8+KJMH3MrDZwJvB4kad/D2xoH/UUcH6644qDme0InBpCGAgQQlgXQlhFHl4X\nQBVg+2TVvi2wBGhInlwXIYRxwIpiTxe/DpoXef7p5O+NB6qb2e6lnT9dib6kRVe10vTaGcXM9gOO\nAj4Cdg8hLAX/MACyeFfKcnsQuBUIAGa2M7AihPBz8udfAnvFFFu67Q8sM7OByaGsx8xsO/Lsuggh\nLAF6AguBxcAqYDKwMk+viw12K3YdbEjmxfPpYsrIp7FuDp5vzGwHvEXEjcnKvvid8Jy+M25mZwFL\nk3/dFJ1qm6/TbqsCxwCPhBCOAX7A/1zPt+uiBl6l7osn8+2BprEGlZkqfB2kK9EvBvYp8rh28rm8\nkfyT9CXgmRDCsOTTSzf8yWVmewD/iSu+NDkZONfMFgCD8SGb3vifnhuuxXy6Nr4EFoUQPk4+fhlP\n/Pl2XZwBLAghfBtCWA+8gl8rNfL0uthgc9fBYmDvIseV+d6kK9H/sugq2dK4BTA8Ta+dKZ4EZoQQ\nehd5bjjw5+T3rYFhxX8pl4QQ7ggh7BNCOAC/BkaHEC4DxuAL7SAP3ocNkn+WLzKzOsmnGgHTybPr\nAh+yOcHMfmNmxsb3Id+uC2PTv26LXgd/ZuO/fzhwOfzSuWDlhiGezZ44XfPokz3te7Nx0VVBWl44\nA5jZycBYYBr+51cA7gAmAC/gn85fAJeEEFbGFWc6mdnpwM0hhHPNbH/8Bn1N4BPgsuRN+5xnZkfi\nN6a3BhYAV+A3JvPqujCzjviH/1r8GvgrXqnmxXVhZoOABLAzsBToCLwKvEgJ14GZ9cGHt34Arggh\nTC71/FowJSKS23QzVkQkxynRi4jkOCV6EZEcp0QvIpLjlOhFRHKcEr2ISI5TohcRyXFK9CIiOe7/\nAc1kGtQCMW5QAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f099c9eb710>"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. Efficiency of MALA and HMC\n",
      "\n",
      "In this exercise we will test the efficiency of MALA and HMC in terms of the distance covered in various situations. All the experiments are done using the same correlated normal target as in Exercise 1, $\\sigma = 0.01$ for MALA and $\\epsilon=0.01$ for HMC.\n",
      "\n",
      "1. Always starting at $x = (0, 0)$, run MALA 100 times for 1 step and 100 times for 100 steps. Compute the average distance covered from the starting point in each case. Compare the 100 step distance with the theoretical random walk scaling of $\\sqrt{n}$ over $n$ steps.\n",
      "2. Always starting at $x = (0, 0)$, run HMC 100 times for 1 step with $L=1$ and 100 times for 1 step with $L=100$. Compute the average distance covered from the starting point in each case. Compare the 100 step distance with the theoretical random walk scaling of $\\sqrt{n}$ over $n$ steps.\n",
      "3. Repeat task 1 starting at $x = (-2, 2)$.\n",
      "4. Repeat task 2 starting at $x = (-2, 2)$.\n",
      "5. In each case, report the ratio of the true scaling ($\\text{average distance with 100 steps}/\\text{average distance with 1 step}$) divided by theoretical scaling ($\\text{theoretical distance with 100 steps}/\\text{theoretical distance with 1 step} = \\sqrt{100}/\\sqrt{1}$) in Moodle. In which cases do the algorithms move more quickly than a random walk would be expected to move?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. NUTS simulation\n",
      "\n",
      "In this exercise we will experiment with the build-tree operation of the NUTS algorithm. We will use a simplified version of the algorithm with the following differences from the original:\n",
      "* We will only extend the path forward instead of choosing the direction randomly\n",
      "* We will check for the termination condition only from the entire newly added path segment, not all subpaths as NUTS really would\n",
      "\n",
      "Your tasks:\n",
      "1. Implement a simplified version of NUTS that simulates $2^{j-1}$ leapfrog steps forward from the most advanced previous position at the $j$th iteration, $j=1,2,\\dots$. The iteration terminates in this simplified algorithm when the first newly simulated position $(x^-, r^-)$ and the last newly simulated position $(x^+, r^+)$ of the newly simulated path segment satisfy\n",
      "$$ (x^+ - x^-) \\cdot r^- < 0 \\quad \\text{or} \\quad (x^+ - x^-) \\cdot r^+ < 0. $$\n",
      "2. Run your algorithm on the 2D correlated normal with $\\rho = 0.998$, starting from the origin $x_0 = (0, 0)$ with momentum $r = (1, 1/2)$ with $\\epsilon=0.05$. How many steps $j$ of the algorithm are executed by the time the termination criterion reached? Compare the length of the simulated trajectory with the number of steps leading furthest from the starting point as determined in Exercise 1.\n",
      "3. NUTS includes a special condition to simulate $u \\sim \\mathrm{Uniform}(0, \\exp(-H(x_0, r_0))$ and ignoring all points $(x,r)$ where $u > \\exp(-H(x, r))$. Assuming $u = 0.98\\exp(-H(x_0, r_0))$ (a pretty extreme value), how many points from the generated trajectory would be kept for the next step of the algorithm i.e. not ignored by this condition?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 4. HMC sampling from a target potential\n",
      "\n",
      "In this exercise we will sample from a 2D density using HMC and NUTS.\n",
      "\n",
      "$$ P^*(x; R) = \\sum_{i=1}^5 \\exp\\left( -\\frac{i}{2} (x-\\mu_i)^T (x - \\mu_i) \\right) $$\n",
      "with $\\mu_1 = (0, 0), \\mu_2 = (R, R), \\mu_3 = (R, -R), \\mu_4 = (-R, R), \\mu_5 = (-R, -R)$. Here $R$ denotes a parameter that specifies the spread of the modes of the distribution which we fix to $R=1.5$. (Please note the $i$ in $\\frac{i}{2}$ inside the $\\exp()$!)\n",
      "\n",
      "1. Run a HMC sampler and NUTS on the problem. Starting with $\\epsilon=1$ and $L = 10$ will give a reasonably calibrated HMC. Compare the estimates of the means produced by the different algorithms. Can you see differences where one would give more accurate results with a smaller number of samples.\n",
      "2. Report the means of $x_1$ and $x_2$ to Moodle. (The required tolerance is $\\pm 0.05$.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 5. HMC sampling for posterior inference\n",
      "\n",
      "Extend the linear regression model of Exercise 2 of Lecture 12 with **normally distributed errors** to handle multiple covariates (input variables) and test it using variables 'FRW', 'AGE' and 'CHOL'.\n",
      "\n",
      "In order to get the correct results, please make sure your data are normalised as in the exercise at the lecture by subtracting the mean and dividing each variable by two standard deviations. After this, the standard deviation of each variable should be $1/2$ and the mean $0$.\n",
      "\n",
      "Draw samples from the posterior distribution of the model using HMC or NUTS when all coefficients $\\beta_i$ have the prior $p(\\beta) = \\prod_i p(\\beta_i) = \\prod_i \\mathcal{N}(\\beta_i;\\; 0, 2^2)$. Make sure your sampler is properly tuned so that results will be reliable.\n",
      "\n",
      "Report your estimates of the posterior means and standard deviations of the intercept ($\\alpha$) and coefficients $\\beta$ for 'FRW', 'AGE' and 'CHOL'.\n",
      "\n",
      "The required tolerance is $\\pm 0.01$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import autograd.numpy as np\n",
      "import autograd\n",
      "\n",
      "def standardize(x):\n",
      "    raise(NotImplementedError(\"Not implemented yet\"))\n",
      "\n",
      "# load the data from CSV file using pandas\n",
      "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
      "# convert the variables of interest to numpy arrays for autograd compatibility\n",
      "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group,\n",
      "#   age and cholesterole level\n",
      "x = np.array(fram[['FRW', 'AGE', 'CHOL']])\n",
      "# target: Systolic blood pressure, examination 1\n",
      "y = np.array(fram['SBP'])\n",
      "\n",
      "xs = standardize(x)\n",
      "ys = standardize(y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}