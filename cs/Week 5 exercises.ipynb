{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:faefb783afb1cddad23ccb9ecef13f59c805eb9a64675e17729ee96358977689"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Week 5 exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. Importance sampling\n",
      "\n",
      "Develop an importance sampler for the bimodal target\n",
      "$$P(x) = \\sum_{i=1}^2 \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2),$$\n",
      "where $\\pi_1 = \\pi_2 = 1/2$ and $\\mu_1 = -1, \\mu_2 = 1$.\n",
      "Use $\\mathrm{Laplace}(0, b)$ with a suitable $b$ as the proposal and evaluate the expectation $\\mathrm{E}[(x-1)^2]$ when\n",
      "1. $\\sigma_1^2 = \\sigma_2^2 = 0.5$.\n",
      "2. $\\sigma_1^2 = \\sigma_2^2 = 0.1$.\n",
      "\n",
      "The required tolerance for the answer is $\\pm 0.1$.\n",
      "\n",
      "In order to estimate the accuracy of your answer, it is recommended to run the sampler a few times and compute the standard deviation of the values you obtain. Monte Carlo error scales as $1/\\sqrt{n}$ with the number of iterations $n$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. Metropolis-Hastings sampling with a fixed proposal\n",
      "\n",
      "Implement Metropolis-Hastings sampling for the bimodal target \n",
      "$$P^*(x) = \\exp(-\\gamma (x^2-1)^2)$$\n",
      "using $\\mathrm{Laplace}(0, 1)$ as the fixed proposal and evaluate the expectation $\\mathrm{E}[(x-1)^2]$ when\n",
      "1. $\\gamma = 8$.\n",
      "2. $\\gamma = 64$.\n",
      "\n",
      "The required tolerance for the answer is $\\pm 0.1$.\n",
      "\n",
      "Hint: It is useful to check your result by plotting a scatter plot of your samples and a contour plot of the logarithm of the target function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. Gibbs sampling for a normal mixture model\n",
      "\n",
      "In this task we will apply Gibbs sampling for sampling the posterior distribution of a normal mixture model for the toy data set used earlier.\n",
      "\n",
      "For two components with shared fixed variance $\\sigma^2$, our model for univariate data $X = (x_1, \\dots, x_n)$ is\n",
      "$$ p(x_i | \\pi, \\mu, \\sigma^2) = \\pi \\mathcal{N}(x_i; \\mu_1, \\sigma^2) + (1-\\pi) \\mathcal{N}(x;\\; \\mu_2, \\sigma^2) $$\n",
      "with parameters $\\pi, \\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2$. The parameters have conjugate prior distributions\n",
      "$$ p(\\pi) = \\mathrm{Beta}(\\pi;\\; \\alpha_\\pi, \\beta_\\pi) \\\\\n",
      "   p(\\mu_k) = \\mathcal{N}(\\mu_k;\\; \\mu_0, \\sigma_0^2), \\quad k = 1,2\n",
      "$$\n",
      "and $p(X | \\pi, \\mu, \\sigma^2) = \\prod_{i=1}^n p(x_i | \\pi, \\mu, \\sigma^2)$\n",
      "\n",
      "The full distribution is intractable but we can derive a Gibbs sampler by augmenting the model with latent variables $z_i$ that indicate which component (1 or 2) was *responsible* for generating $x_i$.\n",
      "\n",
      "If we use $z_i = 1$ to denote the event that the first component was responsible for generating $x_i$ and $z_i = 2$ to denote the event that the latter component was responsible, we can rewrite the model as\n",
      "$$ p(x_i | z_i, \\mu, \\sigma^2) = \\mathcal{N}(x_i; \\mu_1, \\sigma^2)^{2-z_i} \\mathcal{N}(x;\\; \\mu_2, \\sigma^2)^{z_i-1} $$\n",
      "with\n",
      "$$ p(z_i = 1) = \\pi. $$\n",
      "\n",
      "Given $z_i$, the conditional posteriors of $\\mu_k$ can be evaluated in a similar manner as in Problem 5 from Week 4 as\n",
      "$$ p(\\mu_k | Z, X, \\sigma_k^2) = \\mathcal{N}(\\mu_k;\\; m_k, v_k) $$\n",
      "where\n",
      "$$ v_k = \\left( \\frac{1}{\\sigma_0^2} + \\frac{n_k}{\\sigma^2} \\right)^{-1} \\\\\n",
      "   m_k = v_k \\left(\\frac{\\sum_{i: z_i=k} x_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right), $$\n",
      "where $n_k = \\sum_{i=1}^n \\mathbb{1}(z_i = k)$.\n",
      "\n",
      "**NOTE: Please do not apply the update rule given in Problem 5 for Week 4 in general as there was a mistake. Fortunately the mistake only affected the case $\\mu_0 \\neq 0$, so the results of Problem 5 of Week 4 were unaffected.**\n",
      "\n",
      "The conditional posterior for $\\pi$ follows from the conjugate beta model\n",
      "$$ p(\\pi | Z) = \\mathrm{Beta}(\\alpha_\\pi + n_1, \\beta_\\pi + n_2). $$\n",
      "\n",
      "Finally, the conditional posterior for $z_i$ is\n",
      "$$ p(z_i = k | x_i, \\pi, \\mu, \\sigma^2) \\propto \\pi_k \\mathcal{N}(x_i;\\; \\mu_k, \\sigma^2), $$\n",
      "where we have used the simplifying notation $\\pi_1 = \\pi, \\pi_2 = 1-\\pi$.\n",
      "\n",
      "Using these update rules and $\\alpha_\\pi = \\beta_\\pi = 1$, $\\mu_0 = 4, \\sigma_0^2 = 100$ and $\\sigma^2 = 1$, implement and run the Gibbs sampler for the data set loaded below.\n",
      "\n",
      "Report the posterior means and standard deviations of $\\pi, \\mu_1, \\mu_2$ in Moodle. In order to fix the order of the components, $\\mu_1 < \\mu_2$. (If your results are in a different order, please swap $\\mu_1$ and $\\mu_2$, and replace $\\pi$ with $1-\\pi$.)\n",
      "\n",
      "The required tolerance is $\\pm 0.1$ for the means and $\\pm 0.01$ for the standard deviations.\n",
      "\n",
      "Hint: To initialise your sampler, you can set $\\mu,  \\pi$ to some reasonable values and use the update to sample $z_i$. In terms of structuring your code, it may be useful to write separate functions for sampling each of $z, \\pi, \\mu$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import pandas as pd\n",
      "import scipy.special as scs\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
      "data = data.values\n",
      "data = np.array(data[:,0])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 4. Parallel tempering for sampling multimodal distributions\n",
      "\n",
      "In this exercise we will study a variant of the two-dimensional unnormalised target distribution from Lecture 8:\n",
      "$$ P^*(x; R) = \\sum_{i=1}^5 \\exp\\left( -\\frac{i}{2} (x-\\mu_i)^T (x - \\mu_i) \\right) $$\n",
      "with $\\mu_1 = (0, 0), \\mu_2 = (R, R), \\mu_3 = (R, -R), \\mu_4 = (-R, R), \\mu_5 = (-R, -R)$. Here $R$ denotes a parameter that specifies the spread of the modes of the distribution which we fix to $R=6$. (Please note the $i$ in $\\frac{i}{2}$ inside the $\\exp()$!)\n",
      "\n",
      "Your task is to write a sampler to sample from this distribution and evaluate the expectations $\\mathrm{E}[x_1]$ and $\\mathrm{E}[x_2]$.\n",
      "\n",
      "The distribution is strongly multimodal with quite separate modes so you will want to use parallel tempering to ensure your chains can just between the modes.\n",
      "\n",
      "1. Compute $\\log \\frac{P((4, 4); 6)}{P((1, 1); 6)}$ and report it in Moodle.\n",
      "2. Compute the expectations $\\mathrm{E}[x_1]$ and $\\mathrm{E}[x_2]$ and report their values in Moodle.\n",
      "\n",
      "The required tolerance for the expectations is $\\pm 0.3$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 5. Marginal likelihood computation using thermodynamic integration\n",
      "\n",
      "High-throughput genome sequencing produces data that can be summarised as counts of how often a specific sequence feature is observed among the massive number of short sequences produced by the instrument. The Poisson distribution is a natural first choice for these data, but it is in many cases insufficient for describing the variability of the data, and negative binomial distribution is often used as an alternative \"overdispersed Poisson\".\n",
      "\n",
      "In this exercise you will use thermodynamic integration to compute the marginal likelihoods of two models for a data set of counts: a Poisson and a negative binomial. For the Poisson distribution with a suitable prior, the marginal likelihood could be computed analytially, but for the negative binomial this is not possible.\n",
      "\n",
      "The models considered in the problem is as follows:\n",
      "$$ p_1(k_i | \\lambda) = \\mathrm{Poisson}(k_i;\\; \\lambda) = \\frac{\\lambda^{k_i} e^{-\\lambda}}{k_i!} \n",
      "    = \\frac{\\lambda^{k_i} e^{-\\lambda}}{\\Gamma(k_i+1)} \\\\\n",
      "   p_2(k_i | m, a) = \\mathrm{NegBin}(k_i;\\; m, a) = \\frac{\\Gamma(k_i + a^{-1})}{\\Gamma(a^{-1}) \\Gamma(k_i+1)}\n",
      "     \\left( \\frac{m}{m+a^{-1}} \\right)^{k_i} \\left( \\frac{1}{1+ma} \\right)^{a^{-1}}$$\n",
      "and\n",
      "$$ p_i(K | \\theta_i) = \\prod_{i=1}^n p_i(k_i | \\theta_i), $$\n",
      "where $\\theta_i$ denotes the hyperparameters of the corresponding model, i.e. $\\theta_1 = \\{\\lambda\\}, \\theta_2 = \\{m,a\\}$. All parameters $\\lambda, m, a$ are constrained to be positive.\n",
      "\n",
      "Under this parametrisation, the mean of $p_1$ is $\\lambda$ and the mean of $p_2$ is $m$, while the variance of $p_1$ is $\\lambda$ and the variance of $p_2$ is $m + am^2$.\n",
      "\n",
      "We use the following prior distributions:\n",
      "$$ p(\\lambda) = \\mathrm{Gamma}(\\lambda;\\; 1, 1/100) \\\\\n",
      "  p(m) = \\mathrm{Gamma}(m;\\; 1, 1/100) \\\\\n",
      "  p(a) = \\mathrm{Gamma}(a;\\; 2, 2),\n",
      "$$\n",
      "where the pdf of the $\\mathrm{Gamma}(\\alpha, \\beta)$ distribution is\n",
      "$$ \\mathrm{Gamma}(x;\\; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} \\exp(-\\beta x), $$\n",
      "$p(\\theta_1) = p(\\lambda)$ and $p(\\theta_2) = p(m) p(a)$.\n",
      "\n",
      "Your task is to use thermodynamic integration to evaluate the log-marginal-likelihoods $\\log p(K | \\mathcal{M}_i)$ for the two models:\n",
      "$$ \\mathcal{M}_1: p_1(k_i | \\lambda) = \\mathrm{Poisson}(k_i;\\; \\lambda), p(\\lambda) = \\mathrm{Gamma}(\\lambda;\\; 1, 1/100) \\\\\n",
      "\\mathcal{M}_2: p_2(k_i | m, a) = \\mathrm{NegBin}(k_i;\\; m, a), p(m) = \\mathrm{Gamma}(m;\\; 1, 1/100), p(a) = \\mathrm{Gamma}(a;\\; 2, 2) $$\n",
      "and the data set $K$ of integers loaded below.\n",
      "\n",
      "1. Implement a Metropolis-Hastings sampler to sample the parameters $\\theta_i$ for the two models from the tempered unnormalised posterior distributions $p_\\beta^*(\\theta_i | K) = p(K | \\theta_i)^\\beta p(\\theta_i)$. Remember that the parameters need to be positive so you will need to use the $\\log$-transformation to transform them to an unbounded space. Remember to tune your proposal so that you get a not-too-extreme acceptance rate for all $\\beta$ you need.\n",
      "2. Compute the marginal likelihoods for the two models using thermodynamic integration. The range of $\\beta$ values used in Problem 1 of Lecture 10: `np.concatenate((np.array([0.0]), np.logspace(-5, 0, 20)))`, should be sufficient.\n",
      "3. Report $\\log p(K | \\mathcal{M}_1)$ and $\\log p(K | \\mathcal{M}_2)$ to Moodle.\n",
      "\n",
      "The required tolerance for the marginal likelihood values is $\\pm 1$.\n",
      "\n",
      "Hint: there are multiple alternative parametrisations for gamma and negative binomial distributions. Be careful to check that you are using the correct parametrisation if using external code instead of implementing these yourself!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "import scipy.special as scs\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/countdata.txt', sep='\\t', header=None)\n",
      "data = data.values\n",
      "data = np.array(data[:,0])\n",
      "print(np.mean(data), np.var(data))\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}