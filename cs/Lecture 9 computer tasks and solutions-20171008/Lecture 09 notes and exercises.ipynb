{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "",
  "signature": "sha256:f37f4d73be1afd9206ed8ddfc27e8a6da5f9babc2178a7bf855935060eb0bc2a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "University of Helsinki, Department of Mathematics and Statistics  \n",
      "MAST32001 Computational Statistics I, Autumn 2017  \n",
      "Antti Honkela  \n",
      "\n",
      "# Lecture 9: Further (Markov chain) Monte Carlo techniques\n",
      "\n",
      "## More MCMC\n",
      "\n",
      "Today we will consider some futher MCMC techniques:\n",
      "* Examples of when the $\\frac{q(x ; x')}{q(x;x')}$ term in the acceptance probability is essential\n",
      "* Blocking: proposals that only change a (small) subset of dimensions at a time.\n",
      "\n",
      "## Other MCMC considerations\n",
      "\n",
      "* Do not use MCMC unless you have to: if you can perform the integral or parts of it analytically, do so\n",
      "* Sometimes it helps to augment the model with additional variables (e.g. Gibbs sampling for latent variable models). There will be an example in the weekly exercises...\n",
      "\n",
      "## Importance sampling\n",
      "\n",
      "Importance sampling can be used to evaluate expectations over a difficult density $f(x)$ by drawing samples from an easy-to-sample surrogate distribution $g(x)$ and weighting them by the ratio of the true density and the surrogate density. In equations:\n",
      "$$ I_h = \\mathrm{E}_{f(x)}[h(x)] = \\int h(x) f(x) \\mathrm{d}x = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\mathrm{d}x = \\mathrm{E}_{g(x)}\\left[h(x) \\frac{f(x)}{g(x)}\\right]. $$\n",
      "The corresponding Monte Carlo approximation is\n",
      "$$ I_h \\approx \\sum_{i=1}^n h(x_i) \\frac{f(x_i)}{g(x_i)} = \\sum_{i=1}^n h(x_i) w_i, $$\n",
      "where $x_i \\sim g(x)$.\n",
      "\n",
      "In order to apply importance sampling, it is necessary to have\n",
      "$$ g(x) = 0 \\quad \\Rightarrow \\quad h(x)f(x) = 0. $$\n",
      "\n",
      "Good importance sampling estimators have low variance. In particular, it is important that the variance is finite, which requires that\n",
      "$$ \\mathrm{E}_g\\left[ h^2(x) \\frac{f^2(x)}{g^2(x)} \\right] = \\int h^2(x) \\frac{f^2(x)}{g(x)} \\mathrm{d}x < \\infty. $$\n",
      "This is guaranteed for example if\n",
      "$$ \\mathrm{Var}_f[h(X)] < \\infty, \\quad \\text{and} \\quad \\frac{f(x)}{g(x)} \\le M, \\forall x$$\n",
      "for some $M > 0$.\n",
      "\n",
      "Pros:\n",
      "* Easy to apply\n",
      "\n",
      "Cons:\n",
      "* Many importance sampling estimators have infinite variance, making them useless in practice\n",
      "\n",
      "## Further reading:\n",
      "\n",
      "Koistinen: Computational Statistics (Part 1: Sec. 4.7)  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 1. MCMC with an asymmetric proposal\n",
      "\n",
      "In this exercise we will study MCMC sampling in a discrete space, the set of numbers $\\{1, 2, \\dots, 21\\}$.\n",
      "\n",
      "The target distribution will be uniform distribution \n",
      "$$p(n) = \\begin{cases} 1/21 \\text{ when } 1 \\le n \\le 21 \\\\ 0 \\text{ otherwise}. \\end{cases}$$\n",
      "over the points, and the proposal is \n",
      "$$q(n'; n) = \\begin{cases} 1/2 \\text{ when } |n' - n| = 1 \\\\ 0 \\text{ otherwise}. \\end{cases}$$\n",
      "\n",
      "1. Implement the sampler and test it by running it for 100000 iterations.\n",
      "2. As jumps from 1 to 0 and from 21 to 22 are always rejected, we could try optimising the sampler by using an alternative proposal\n",
      "$$ q'(n'; n) = \\begin{cases} 1 \\text{ when } (n, n') \\in \\{(1, 2), (21, 20)\\} \\\\ 1/2 \\text{ when } 2 \\le x \\le 20 \\wedge |n' - n| = 1 \\\\ 0 \\text{ otherwise}. \\end{cases}$$\n",
      "Implement this proposal but do not include the $\\frac{q(n;n')}{q(n';n)}$ term in the acceptance ratio. Which distribution is the sampler now sampling from?\n",
      "3. Fix the sampler by using the full acceptance rule. Can you get it to sample from the uniform distribution?\n",
      "\n",
      "Hint: $\\log 0 = -\\infty$ can be produced with `np.inf` for code working with log-probabilities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 2. MCMC with a fixed proposal\n",
      "\n",
      "MCMC sampling can also be used in a way that the proposal $q(x';x)$ is independent of $x$. This resembles rejection sampling, except we do not need an explicit rejection threshold.\n",
      "\n",
      "Write a Metropolis-Hastings sampler for the $\\mathcal{N}(0,1)$ distribution using $\\mathrm{Laplace}(0, 1)$ as the fixed proposal.\n",
      "\n",
      "Compute the expectation $\\mathrm{E}[x^2]$ and $\\mathrm{E}[x^4]$ for your samples, standard normal and the Laplace proposal to check that if they match.\n",
      "\n",
      "Hint: It is vital to include the $\\frac{q(x;x')}{q(x';x)}$ term in the acceptance ratio now."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 3. Importance sampling\n",
      "\n",
      "Importance sampling can be used to evaluate expectations over a difficult density $f(x)$ by drawing samples from an easy-to-sample surrogate distribution $g(x)$ and weighting them by the ratio of the true density and the surrogate density. In equations:\n",
      "$$ I_h = \\mathrm{E}_{f(x)}[h(x)] = \\int h(x) f(x) \\mathrm{d}x = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\mathrm{d}x = \\mathrm{E}_{g(x)}\\left[h(x) \\frac{f(x)}{g(x)}\\right]. $$\n",
      "The corresponding Monte Carlo approximation is\n",
      "$$ I_h \\approx \\sum_{i=1}^n h(x_i) \\frac{f(x_i)}{g(x_i)} = \\sum_{i=1}^n h(x_i) w_i, $$\n",
      "where $x_i \\sim g(x)$.\n",
      "\n",
      "Evaluate the expectations $\\mathrm{E}[x^2]$ and $\\mathrm{E}[x^4]$ over standard normal $\\mathcal{N}(0, 1)$ using importance sampling with Laplace distribution $\\mathrm{Laplace}(0,1)$ as the surrogate density."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "## 4. Blocking in MCMC\n",
      "\n",
      "Metropolis-Hastings sampling in high dimensions is difficult because one often needs to use very short jumps to keep the acceptance rate manageable. A common technique for managing this is *blocking*, where each proposal only updates a subset of the dimensions. This can lead to higher acceptance rates on longer jumps, but can also lead to difficulties if there are strong dependencies across block boundaries.\n",
      "\n",
      "In this exercise we will experiment with blocking. We will sample a $d=100$ dimensional multivariate normal with two covariances:  \n",
      "a) $\\Sigma = I_d$  \n",
      "b) A non-diagonal covariance. You can generate such a covariance with $\\Sigma = A A^T + \\epsilon I_d$, where all entries of $A \\in \\mathbb{R}^{d \\times k}$ follow $\\mathcal{N}(0, 1)$. $k=2$ and $\\epsilon=0.1$ give some reasonable results, but feel free to explore other values too.\n",
      "\n",
      "To numerically test your samplers, you can check how accurately they can estimate the mean (0) of the proposal, especially when initialised to a non-zero value.\n",
      "\n",
      "1. Apply Metropolis-Hastings sampling to sample from $\\mathcal{N}(0, \\Sigma)$ for high $d$, e.g. $d=1000, d=10000$ using a normal proposal $q(x'; x) = \\mathcal{N}(x, \\sigma^2 I_d)$. Observe how the dimensionality affects the width of the proposal required for a reasonable acceptance rate. Plot a trace of the samples along a few dimensions to see how the sampler proceeds.\n",
      "2. Re-implement the same sampler using blocking to only propose moves that change a subset of the dimensions at the time and rotate between the dimensions. Experiment with different block sizes.\n",
      "\n",
      "Hint: If you are up to a little coding exercise, implement the blocked proposal yourself. If you do not feel up to the task, feel free to use the example implementation below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as npr\n",
      "\n",
      "class blocked_normal_proposal:\n",
      "    \"\"\"A class for a blocked normal proposal.\n",
      "      p = blocked_normal_proposal(d, blocks)\n",
      "    creates a proposal object yielding d-dimensional normal random numbers\n",
      "    where each of the \"blocks\" blocks in turn follows N(0, I) when calling\n",
      "      p.draw()\n",
      "    \"\"\"\n",
      "    def __init__(self, d, blocks):\n",
      "        self.d = d\n",
      "        self.blocks = blocks\n",
      "        self.curblock = 0\n",
      "    \n",
      "    def next_block_indices(self):\n",
      "        I = np.zeros(self.d, np.bool)\n",
      "        I[((self.curblock*self.d)//self.blocks):(((self.curblock+1)*self.d)//self.blocks)] = True\n",
      "        self.curblock = (self.curblock + 1) % self.blocks\n",
      "        return I\n",
      "    \n",
      "    def draw(self):\n",
      "        res = np.zeros(self.d)\n",
      "        I = self.next_block_indices()\n",
      "        res[I] = npr.normal(size=np.sum(I))\n",
      "        return res\n",
      "\n",
      "# Use this to get help:\n",
      "# ? blocked_normal_proposal\n",
      "p = blocked_normal_proposal(6, 3)\n",
      "print(p.draw())\n",
      "print(p.draw())\n",
      "print(p.draw())\n",
      "print(p.draw())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}